---
title: "Prediction of Infectious Disease Epidemics via Feature-Weighted Density Ensembles"
author: "Evan L Ray, Krzysztof Sakrejda, Nicholas G Reich"
output: 
    pdf_document:
        keep_tex: false
        citation_package: natbib
        fig_caption: true
date: "`r format(Sys.time(), '%B %Y')`"
header-includes:
   - \input{GrandMacros.tex}
   - \usepackage{setspace}\onehalfspacing
   - \usepackage{lineno}\linenumbers
   - \renewcommand{\familydefault}{cmss}
bibliography: feature-weighted-ensembles.bib
---

```{r init, include = FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(awes)
```

# Abstract


# Introduction

The practice of combining predictions from different models has been used for decades by climatologists and geophysical scientists. These methods have subsequently been adapted and extended by statisticians and computer scientists in a wide array of applications. In recent years, these "ensemble" forecasting approaches are frequently used among the top methods in prediction challenges. 

Ensembles are a natural choice for noisy and complex interdependent systems that evolve over time. In this setting, no one model is likely to be able to predict the full dynamics of a complex system. Instead "specialist" or "component" models could be relied on to capture distinct features or signals from a system which, when combined, represent a complete range of possible outcomes. 

Using component methods that generate predictive densities for outcomes of interest, we have developed a feature-weighted density ensemble method that estimates model weights as functions of observed data. Our approach fuses aspects of different ensemble methods: it uses model "stacking" \citep{Wolpert1992}, combines predictive densities as in Bayesian Model Averaging \citep{Madigan1998}, and estimates model weights based on features of the system \citep{Sill2009} using gradient tree boosting \citep{Hastie2011}.

To illustrate this method, we present time-series forecasts for infectious disease, specifically for influenza in the U.S.
For millenia, infectious diseases have been one of the central existential threats to humankind. Even in today's modern and scientific world, the spectre of a global pandemic appears to be a real possibility. The international significance of emerging epidemic threats in recent decades, including HIV/AIDS since the 1980s, SARS in 2003, H1N1 in 2009, and Ebola and MERS in 2014-2015, has also increased public awareness of the importance of understanding and being able to predict infectious disease dynamics. With the revolution in data-driven science also occurring in the past few decades, there is now an increased focus on and hope for using statistics to inform public health policy and decision-making in ways that could impact future outbreaks. Some of the largest public health agencies in the world, including the US Centers for Disease Control and Prevention (CDC) have openly endorsed using models to inform decision making, saying: "with models, decision-makers can look to the future with confidence in their ability to respond to outbreaks and public health emergencies." [ADD CITATION: http://www.cdc.gov/cdcgrandrounds/archives/2016/january2016.htm]

The observation that motivated the development of the method presented in this manuscript was seeing how certain prediction models for infectious disease consistently performed better than other models at certain times of year. We set out to determine whether past model performance could improve predictions using feature-weighted model stacking. We observed that early in the U.S. influenza season, simple models of historical incidence often outperformed more standard time-series prediction models such as a seasonal auto-regressive integrated moving average (SARIMA) model. However, in the middle of the season, the time-series models showed improved accuracy.

Using seasonal influenza outbreaks in the US health regions as a case-study, we developed and applied our feature-weighted ensemble model to predicting several features of the influenza season at each week during the season. 



# Methods

## Data

We obtained publicly available data on seasonal influenza activity in the United States between 1997 and 2016 from the U.S. Centers for Disease Control and Prevention (CDC) (\ref{fig:raw-data}). 
For each of the 10 Health and Human Services regions in the country in addition to the nation as a whole, the CDC calculates and publishes each week a measure called the weighted influenza-like illness (wILI) index. 
The wILI for a particular region is calculated as the average proportion of doctor visits with influenza-like illness for each state in the region, weighted by state population. 
During the CDC-defined influenza season (between MMWR week 40 of one year and 20 of the next year), the CDC publishes updated influenza data on a weekly basis. 
This includes "current" wILI data from two weeks prior to the reporting date, as well as updates to previously reported numbers as new data becomes available. 
For this analysis, we use only the final reported wILI measures to train and predict from our models.

The CDC defines the influenza season onset as the first of three successive weeks of the season for which wILI is greater than or equal to a threshold that is specific to the region and season. 
This threshold is the mean percent of patient visits where the patient had ILI during low incidence weeks for that region in the past three seasons, plus two standard deviations.\citep{cdc2016}
The CDC provides historical threshold values for each region going back to the 2007/2008 season.\citep{cdc2016-baselines}

Additionally, we define two other metrics specific to a region-season. The peak week is the week at which the maximum wILI for the season is observed. The peak incidence is the maximum observed wILI measured in a season.

## Component models
We used three component models to generate probabilistic predictions of these three quantities of interest. The first model was a seasonal average model that utilized kernel density estimation to estimate a predictive distribution for each target. The second model utilized kernel conditional density estimation to create predicted trajectories of incidence. By aggregating across all trajectories, we constructed predictive distributions for each target. The third model used a standard seasonal auto-regressive integrated moving average (SARIMA) implementation and model selection approach to fit a model to the data. All models were fit independently on data within a particular region.

### Kernel Density Estimation (KDE)
The simplest of the component models uses kernel density estimation (KDE) to estimate a predictive distribution of future values based on observed data from previous seasons within the region of interest. 
Let the vector of observations for one of the targets be $\by_{1:K}$, so for example, this might represent the vector of the peak wILI values from the $K$ training seasons.
We used Gaussian kernels and the default KDE settings from the `density` function in the `stats` package for R \citep{Rcore2016}. 
To create an empirical predictive distribution of size $N$ from a KDE fit based on a data vector of length $K$ ($\by_{1:K}$), we first drew $N$ samples (with replacement) from $\by_{1:K}$, yielding a new vector $\tilde \by_{1:N}$. 
We then drew a single psuedo-random deviates from each of $N$ Gaussian distributions centered at $\tilde \by_{1:N}$ with the bandwidth estimated by the KDE algorithm. 
These sampled points then make up the empirical predictive distribution from a KDE model.
For the onset week outcome, we added the probability of no onset occuring within a season as [[TODO: XXXX]] and then standardized this along with the sampled probabilities so they summed to 1.
<!--for week-based outcomes: we just removed samples that were <=0 and >52. Arbitrary but likely not a big deal. -->

It is important to note that the predictions from this model do not change as new data are observed over the course of the season.
This is a strictly seasonal model, only ever predicting a distribution that relates to the average observations in past seasons.


### Kernel Conditional Density Estimation (KCDE)
We use a method called kernel conditional density estimation (KCDE) to estimate predictive distributions for each target. 
Using a kernel-based approach to nearest-neighbors regression, KCDE fits separate predictive densities for each future week in the season. 
Our implementation for this paper conditioned on recent past incidence and week of season, only using information from the time series itself.
In order to predict seasonal quantities (onset, peak timing, and peak incidence), we use a copula to model dependence among those individual predicitive densities for each week, thereby obtaining a joint predicitive density, or a distribution of incidence trajectories in all future weeks.
For each predicted trajectory, the peak week, peak height, and onset week are computed and these are aggregated to create predictive distributions for each target respectively (see \citep{Ray2016} for details).

[[TODO: (Optional) add 2-3 sentences with a few more details on methodology, etc... cross-validation, bandwidth selection, periodic kernel, data on log-scale?, etc...]]

### Seasonal auto-regressive integrated moving average (SARIMA)
We fit seasonal ARIMA models \citep{Box2015} to wILI observations transformed to be on the natural log scale. We used the stepwise procedure from the `auto.arima` function in the `forecast` package for R to select the order of first- and seasonal-differencing as well as the specification of the order of auto-regressive and moving average terms for the SARIMA model.\citep{Hyndman2008} Additionally, data were Box-Cox transformed prior to modeling. 

[[TODO: Specifications for forecast algorithm - what method used?]] 
Similar to KCDE, forecasts were obtained as trajectories of wILI values over the rest of the season and predictive distributions of the targets were computed as described above.

## Model training and cross-validation

We used data from 14 seasons (1997/1998 through 2010/2011) to train the models. 
Data from five seasons (2011/2012 through 2015/2016) were held out when fitting the models and used exclusively in the testing phase. 
Care was taken to ensure that our test predictions were made only once, to avoid overfitting our models \citep{Hastie2011}.

For each model-region pair, we fitted the model 14 times, each time excluding one training season during the fitting.
Then for each season-week within the left-out season, we generated a set of three predictive distributions, one for each of the prediction targets. 
In total, this generated nearly 45,000 predictive distributions across the entire training phase: 3 targets x 3 models x 11 regions x 14 training seasons x 32 weeks per season.
(Several of the training seasons had 33 weeks per season because they contained an MMWR week 53.)

Each predictive distribution was represented by probabilities assigned to bins associated with different possible outcomes.
For onset week, the bins are represented by integer values for each possible season week plus a bin for "no onset". 
For peak week, the bins are represented by integer values for each possible season week.
For peak incidence, the bins are intervals of length 0.1 from [0, 0.1), [0.1, 0.2), ..., [12.9, 13.0), with a final bin that captures all high incidence values, [13.0, $\infty$].
[[TODO: Is this capturing all of the subtlety of rounding in bins? ]]

Using the eventually observed value for each of the three outcomes, we then calculated the log-score achieved by each of predictive distributions across the entire training period.
The log score is a proper scoring rule\citep{Gneiting2007}, calculated in our setting as the natural log of the probability assigned to the bin containing the true observation. 


## Ensemble model overview

By definition, ensemble models combine a set of models into one single model. 
While there are many different methods for combining models, all ensemble models discussed in this paper use a model averaging methodology called "stacking", whereby predictions from different models are averaged to obtain the ensemble prediction.\citep{Hastie2011} 
In particular, we considered only models that create probabilistic predictive distributions as described above.
We implemented several simple stacking procedures to use as reference ensemble methods for our proposed stacking methodology. 
This ensured that the new methodology could be compared with less complex methods as a baseline. 

We propose a novel framework for estimating _feature-dependent weights_ for a stacked ensemble model. 
By _feature-dependent_ we mean that the optimal weights associated with different component models are driven by observed features or covariates.
Although we illustrate the method in the context of time-series predictions, the method could also be used for any model using stacking to generate predictive distributions.
Features could include observed data from the system being predicted (such as recent wILI measurements or the time of year at which predictions are being made), observed data from outside the system (for example, recent weather observations), or features of the predictions themselves (e.g. summaries of the predictive distribution itself, such as a measure of spread in the distribution, or the time since a predicted peak).
Based on exploration of training phase data and _a priori_ knowledge of the disease system, we chose three features of the system to illustrate the proposed "feature-weighting" methodology: 
week of season, 
model confidence (defined as the minimum number of predictive distribution bins required to cover 90% probability), and 
wILI measurement at the time of prediction.

All of the ensemble frameworks implemented here can be described using a single set of notation.
Let $f_m(y|\bX)$ denote the predictive density from model $m$ for the value of the scalar random variable $Y$ conditional on observed variables $\bX$.  
For example, $Y$ could represent the peak incidence for a given season.
In the context of repeated predictions from a time-series, the covariate vector $\bX$ may include time-dependent covariates, e.g. the week at which the prediction is made. 
However to introduce the method more generally, we can think of $\bX$ as being any set of covariates that may influence which model may more accurately predict the true outcome.

The combined predictive density for a particular target in a given region can be written as 
\begin{equation}
f(y|\bX) = \sum_{m = 1}^M \pi_m f_m(y|\bX). \label{eqn:EnsembleModel} 
\end{equation}
In Equation \eqref{eqn:EnsembleModel} the $\pi_m$ are the model weights. These $\pi_m$ could either be taken as _a priori_ fixed values (e.g. $\pi_m = 1/M$, for all $m$) or it could take a more complicated functional form such as 
\begin{equation}
\pi_{m}(\bX) = \frac{\exp\{\rho_m(\bX)\}}{\sum_{m' = 1}^M \exp\{\rho_{m'}(\bX)\}}  \label{eqn:PiSoftmaxRho} 
\end{equation}
where $\rho_m(\bX)$ is a function that depends on observed data and could be estimated using various techniques. 
In either setting, the weights must be non-negative and sum to $1$ across $m$.  

Using component model predictions from the training phase, we used four distinct methodologies to create or define weights to use for the stacking models. 

1. Equal model weights, i.e. $\pi_m = 1/M$. In this scenario, each model contributes the same weight for each target and for all values of $\bX$.

2. Constant model weights, i.e. $\pi_m(\bX) = c_m$, a constant, where $\sum c_m = 1$ but the constants are not necessarily the same for each model. These weights are estimated using the degenerate estimation-maximization (EM) algorithm.\citep{Lin2004} 

3. Feature-weighted, i.e. $\rho_m(\bX)$ are estimated using gradient boosting with features including week of the season and model confidence. [[TODO: do we include a season-week only model?]]

4. Feature-weighted and smoothed, i.e. $\rho_m(\bX)$ as above but with regularization tuning parameters chosen for the gradient boosting that penalize large fluctuations in the estimated $\rho_m(\bX)$. In these models, we include the same features as in 3 above along with the most recent wILI value. Details on both feature-weighted frameworks are described below.

[[TODO: create a table defining models and highlighting which covariates are used]]

All in all, this leads to [[TODO: 6 or 7]] ensemble models fitted to the training phase data. 
Each of the ensemble models, along with the three component models, are used to generate predictions in every season-week of each of the five testing seasons, assuming perfect reporting. 
These predictions are then used to evaluate the prospective predictive performance of each of the ensemble methods. 
In total, we evaluate [[TODO: total_ensemble_models + total_component_models]] models in 11 regions over 5 years and 3 targets of interest. 

## Feature-weighted stacking framework

This method was motivated by our observation in earlier work that there are nonlinearities in the relative performance of different models as a function of the time of year at which predictions are made. 
The framework described above in Equation \eqref{eqn:rhoSplineExpansion} could be used to capture these nonlinearities.
Specifically, we estimate the functions $\rho_m(\bX)$ using gradient tree boosting.

Gradient tree boosting uses a forward stagewise additive modeling algorithm to iteratively and incrementally construct a series of regression trees that, when added together, create a function designed to minimize a given loss function. 
In our application, the algorithm builds up the $\rho_m(\bX)$ that minimize the negative log-score of the stacked prediction $f(y|\bX)$. 

Parameters of the algorithm: depth of trees (number of splits within each tree?), number of boosting iterations (is this $N$, below? like a total number of trees), "learning rate".

Specifically, we define a single tree as
\begin{equation}
T(\bX; \Theta) = \sum_{j=1}^J \gamma_j I(\bX \in R_j)
\end{equation}
where the $R_j$ are a disjoint set of regions that comprise a partition of the space of all predictor variables. 
Additionally, the model takes $\Theta = \{R_j, \gamma_j \}_{1:J}$ as parameters with $J$ as a meta-parameter.
Then the sum of the $N$ trees represents the function
\begin{equation}
\rho_m(\bX) = \sum_{n=1}^N T(\bX; \Theta_n).
\end{equation}
[[TODO: check formulation above, including mention of whether estimation for each model m is done separately or jointly]]

Additionally, we have introduced regularization into our estimation of the $\rho_m$ functions. 
For now, weight regularization is done through three parameters; optimal values of those parameters are selected through cross-validation and a single best weighting scheme is selected.  
The parameters are the depth of the regression trees used in the gradient tree boosting process, the number of boosting iterations, and the learning rate.

We ensure that the the $\pi_m$ are non-negative and sum to 1 by parameterizing the $\pi_m$ in terms of the softmax transformation of real-valued functions $\rho_m$ in Equation \eqref{eqn:PiSoftmaxRho}.  
We avoid identifiability problems by fixing $\rho_M(\bX) = 0$ for all $\bX$.  
Therefore for models $m \in \{1, ..., M-1\}$, $\rho_m(\bX) > 0$ indicates that model $m$ has more weight than the baseline model for predictions at the given value of $\bX$. 

## Model testing

<!-- Two feature-weighted (changing by season_week only, estimated separately by metric and region) -->
<!--   3a. season_week -->
<!--   3b. season_week*model_confidence -->
<!-- Three feature-weighted with regularization parameters chosen via cross-validation: -->
<!--   4a. season_week -->
<!--   4b. season_week*model_confidence -->
<!--   4c. season_week*model_confidence*lag1_ili -->

<!-- X = (week of season, model confidence, recent incidence) -->

<!-- 8 methods (1 equal-bin-weight + 3 component + 4 ensemble) / 3-7 metrics / 5 test seasons -->


<!-- To prevent the stacking model from overfitting the training data, we estimate the model parameters by optimizing a penalized loss function where the penalty encourages the splines to be smooth.  The underlying unpenalized loss function is a measure of the quality of the final predictive distribution $f_t(y_t)$, obtained by cross-validation.  Because model performance will be evaluated using log scores, we will use negative cross-validated log scores as the loss function during parameter estimation: -->
<!-- The final predictions are obtained as a linear combination of the predictions from these component models.  The model weights depend on the week of the season in which the predictions are made.  We represent these weights as the softmax transformation of latent functions rho_ilt(season week) where i = 1, ..., 3 indexes the component model, l = 1, ..., 11 indexes the location (national or region 1 through 10), and t = 1, ..., 7 indexes the prediction target.  We estimate those latent functions rho_ilt via gradient tree boosting, optimizing leave-one-season-out crossvalidated log scores (using the definition of log scores specified for this competition). -->

To evaluate overall model performance, we computed the average log-score for each model across all regions and/or test seasons.
Additionally for the peak wILI targets, we computed the average log-score for each model in the test seasons separately before and after the peak week.
Similarly for the onset week target, we computed the average log-score for each model in the test seasons before and after the onset week.

## Software and code

We used `r R.version.string` for all analyses.\citep{Rcore2016}
All data and code used for this analysis is freely available in an R package online at https://github.com/reichlab/adaptively-weighted-ensemble and may be installed in R directly.
Predictions generated in real-time with early development versions of this model during the 2016/2017 influenza season may be viewed at https://reichlab.github.io/flusight/.
To maximize reproducibility of our work, we have set seeds prior to running code that relies on stochastic simulations using the `rstream` package.\citep{Leydold2015}  
Additionally, the manuscript itself was dynamically generated using RMarkdown.

# Results

#### Figure: show the flu data \label{fig:raw-data}

```{r raw_data_plot, echo=FALSE, fig.height=8, fig.cap="Plot of influenza data.  The full data include observations aggregated to the national level and for 10 smaller regions.  Here we plot only the data at the national level and in two of the smaller regions; data for the other regions are qualitatively similar.  Missing data are indicated with vertical grey lines.  The vertical red dashed lines indicates the cutoff time between the training and testing periods; 5 seasons of data were held out for testing."}
flu_data$region[flu_data$region == "X"] <- "National"
flu_data$region <- factor(flu_data$region,
  levels = c("National", paste0("Region ", 1:10)))

regions_to_plot <- c("National", "Region 1", "Region 7")
#regions_to_plot <- c("National", paste0("Region ", 1:10))

train_cutoff_ind <- min(which(flu_data$season == "2011/2012"))
train_cutoff_time <- flu_data$time[train_cutoff_ind]
train_cutoffs <- data.frame(
  region = regions_to_plot,
  train_cutoff_time = as.numeric(as.Date(train_cutoff_time))
)

ggplot() +
  geom_line(aes(x = as.Date(time), y = weighted_ili),
    data = flu_data[flu_data$region %in% regions_to_plot & flu_data$season %in% paste0(1997:2015, "/", 1998:2016), ]) +
  geom_vline(aes(xintercept = as.numeric(as.Date(time))),
    colour = "grey",
    data = flu_data[is.na(flu_data$weighted_ili) & flu_data$region %in% regions_to_plot & flu_data$season %in% paste0(1997:2015, "/", 1998:2016), ]) +
  geom_vline(aes(xintercept = train_cutoff_time),
    colour = "red",
    linetype = 2,
    data = train_cutoffs) +
  facet_wrap(~ region, ncol = 1) +
  scale_x_date() +
  xlab("Time") +
  ylab("Weighted Proportion of Doctor's Office Visits\nwith Influenza-like Illness\n") +
  ggtitle("Influenza Data by Region") +
  theme_bw(base_size = 11)
```

#### Figure: 9 panel grid (3 component models x 3 metrics) showing a solid line for each region that represents the average log score across all seasons \label{fig:log-scores}

```{r log_scores_v1, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="version 1 of log scores plot: vertical axis is mean log score across all seasons from 1999/2000 - 2015/2016 (both train and test, drop first two train seasons for all models since sarima made no predictions then and they were weird), faceted by prediction target and component model.  separate line for each region"}
log_scores <- bind_rows(
    assemble_predictions(
      preds_path = "../../inst/estimation/loso-predictions",
      regions = c("National", paste0("Region", 1:10)),
      models = c("kde", "kcde", "sarima"),
      prediction_targets = c("onset", "peak_week", "peak_inc"),
      prediction_types = c("log_score")
    ),
    assemble_predictions(
      preds_path = "../../inst/evaluation/test-predictions",
      regions = c("National", paste0("Region", 1:10)),
      models = c("kde", "kcde", "sarima"),
      prediction_targets = c("onset", "peak_week", "peak_inc"),
      prediction_types = c("log_score")
    )
  ) %>%
  gather_("prediction_target", "log_score",
    paste0(c("onset", "peak_week", "peak_inc"), "_log_score")) %>%
  mutate(prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10))

mean_log_scores <- log_scores %>%
  filter(analysis_time_season %in% paste0(1999:2015, "/", 2000:2016)) %>%
  group_by(model, region, prediction_target, analysis_time_season_week) %>%
  summarize(mean_log_score = mean(log_score))

ggplot(mean_log_scores) +
  geom_line(aes(x = analysis_time_season_week, y = mean_log_score, group = region)) +
  facet_grid(model ~ prediction_target) +
  theme_bw()
```

```{r log_scores_v2, echo=FALSE, fig.height=8, fig.cap="version 2 of log scores plot: data subset to national only, just training seasons (except for first two as above), facetted by season and prediction target"}
ggplot(log_scores[log_scores$region == "National" & log_scores$analysis_time_season %in% paste0(1999:2010, "/", 2000:2011), ]) +
  geom_line(aes(x = analysis_time_season_week, y = log_score, colour = model, linetype = model)) +
  facet_grid(analysis_time_season ~ prediction_target) +
  theme_bw()
```

```{r log_scores_v3, echo=FALSE, fig.height=8, fig.cap="version 3 of log scores plot: data subset to the same 3 regions used in the first plot above, all seasons (both test and tarining, except for first two as above), facetted by region and prediction target, mean log scores across all seasons"}
mean_log_scores <- log_scores %>%
  filter(analysis_time_season %in% paste0(1999:2015, "/", 2000:2016)) %>%
  group_by(model, region, prediction_target, analysis_time_season_week) %>%
  summarize(mean_log_score = mean(log_score))

regions_to_plot <- c("National", "Region1", "Region7")

ggplot(mean_log_scores[mean_log_scores$region %in% regions_to_plot, ]) +
  geom_line(aes(x = analysis_time_season_week, y = mean_log_score, colour = model, linetype = model)) +
  facet_grid(region ~ prediction_target) +
  theme_bw()
```

#### Figure: Example of log-scores and estimated weights from one region by season-week (x) for each model (color). Panel 1 has log-scores (y); Panel 2 has estimated weights (y) using degenerate EM, feature-weighted, and feature-weighted + smoothed \label{fig:example-weights}

#### Figure: test phase summary: 3 row-facets, one for each target, each model (color?) year (x) is a point with log-score (y) \label{fig:test-phase-log-score}

#### Figure: test phase summary: 3 row-facets, one for each target, each model (color?) year (x) is a point with MAE (y) \label{fig:test-phase-MAE}


# Conclusion

### Achievements

 - developed ensemble framework that makes [[better]] predictions on average than component models
 - ensemble method uses novel method to estimate feature-dependent weights
 - predictions disseminated and updated weekly
 
### Strengths/novelty

 - feature-weighting works regardless of discrete/continuous covariate
 - operates on predictive distributions, not just point-estimates
 - General framework (not just time-series or infectious disease) 
 - framework could be used to answer epidemiologically relevant questions: when do certain models contain information that substantially improve predictions, e.g. adding weather, mechanistic models, strain specific models, etc...

### Limitations

 - illustration only includes simple models and simple features

# References
 
 
