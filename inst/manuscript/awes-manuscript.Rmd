---
title: "Prediction of Infectious Disease Epidemics via Feature-Weighted Density Ensembles"
author: "Evan L Ray, Krzysztof Sakrejda, Nicholas G Reich"
output: 
    pdf_document:
        keep_tex: false
        citation_package: natbib
date: "`r format(Sys.time(), '%B %Y')`"
header-includes:
   - \input{GrandMacros.tex}
   - \usepackage{setspace}\doublespacing
   - \usepackage{lineno}\linenumbers
   - \renewcommand{\familydefault}{cmss}
bibliography: feature-weighted-ensembles.bib
---


# Abstract


# Introduction

The practice of combining predictions from different models has been used for decades by climatologists and geophysical scientists. These methods have subsequently been adapted and extended by statisticians and computer scientists in a wide array of applications. In recent years, these "ensemble" forecasting approaches are frequently used among the top methods in prediction challenges. 

Ensembles are a natural choice for noisy and complex interdependent systems that evolve over time. In this setting, no one model is likely to be able to predict the full dynamics of a complex system. Instead "specialist" or "component" models could be relied on to capture distinct features or signals from a system which, when combined, represent a complete range of possible outcomes. 

Using component methods that generate predictive densities for outcomes of interest, we have developed a feature-weighted density ensemble method that estimates model weights as functions of observed data. Our approach fuses aspects of different ensemble methods: it uses model "stacking" \citep{Wolpert1992}, combines predictive densities as in Bayesian Model Averaging \citep{Madigan1998}, and estimates model weights based on features of the system \citep{Sill2009} using gradient tree boosting [[need citation]]. 

To illustrate this method, we present time-series forecasts for infectious disease, specifically for influenza in the U.S.
For millenia, infectious diseases have been one of the central existential threats to humankind. Even in today's modern and scientific world, the spectre of a global pandemic appears to be a real possibility. The international significance of emerging epidemic threats in recent decades, including HIV/AIDS since the 1980s, SARS in 2003, H1N1 in 2009, and Ebola and MERS in 2014-2015, has also increased public awareness of the importance of understanding and being able to predict infectious disease dynamics. With the revolution in data-driven science also occurring in the past few decades, there is now an increased focus on and hope for using statistics to inform public health policy and decision-making in ways that could impact future outbreaks. Some of the largest public health agencies in the world, including the US Centers for Disease Control and Prevention (CDC) have openly endorsed using models to inform decision making, saying: "with models, decision-makers can look to the future with confidence in their ability to respond to outbreaks and public health emergencies." [ADD CITATION: http://www.cdc.gov/cdcgrandrounds/archives/2016/january2016.htm]

The observation that motivated the development of the method presented in this manuscript was seeing how certain prediction models for infectious disease consistently performed better than other models at certain times of year. We set out to determine whether past model performance could improve predictions using feature-weighted model stacking. We observed that early in the U.S. influenza season, simple models of historical incidence often outperformed more standard time-series prediction models such as a seasonal auto-regressive integrated moving average (SARIMA) model. However, in the middle of the season, the time-series models showed improved accuracy.

Using seasonal influenza outbreaks in the US health regions as a case-study, we developed and applied our feature-weighted ensemble model to predicting several features of the influenza season at each week during the season. 



# Methods

## Data

We obtained publicly available data on seasonal influenza activity in the United States from the U.S. Centers for Disease Control and Prevention (CDC) from 1997-2016. 
For each of the 10 Health and Human Services regions in the country in addition to the nation as a whole, the CDC calculates and publishes each week a measure called the weighted influenza-like illness (wILI) index. 
The wILI for a particular region is calculated as the average proportion of doctor visits with influenza-like illness for each state in the region, weighted by state population. 
During the CDC-defined influenza season (between MMWR week 40 of one year and 20 of the next year), the CDC publishes updated influenza data on a weekly basis. 
This includes "current" wILI data from two weeks prior to the reporting date, as well as updates to previously reported numbers as new data becomes available. 
For this analysis, we use only the final reported wILI measures to train and predict from our models.

The CDC defines the influenza season onset as the first of three successive weeks of the season for which wILI is greater than or equal to a threshold that is specific to the region and season. 
This threshold is the mean percent of patient visits where the patient had ILI during low incidence weeks for that region in the past three seasons, plus two standard deviations (cite https://www.cdc.gov/flu/weekly/overview.htm).  
The CDC provides historical threshold values for each region going back to the 2007/2008 season (cite https://github.com/cdcepi/FluSight-forecasts/blob/master/wILI_Baseline.csv).

Additionally, we define two other metrics specific to a region-season. The peak week is the week at which the maximum wILI for the season is observed. The peak incidence is the maximum observed wILI measured in a season.

## Component models
We used three component models to generate probabilistic predictions of these three quantities of interest. The first model was a seasonal average model that utilized kernel density estimation to estimate a predictive distribution for each target. The second model utilized kernel conditional density estimation to create predicted trajectories of incidence. By aggregating across all trajectories, we constructed predictive distributions for each target. The third model used a standard seasonal auto-regressive integrated moving average (SARIMA) implementation and model selection approach to fit a model to the data. All models were fit independently on data within a particular region.

### Kernel Density Estimation (KDE)
The simplest of the component models uses kernel density estimation (KDE) to estimate a predictive distribution of future values based on observed data from previous seasons within the region of interest. 
The predictions from this model do not change as new data are observed over the course of the season. 
Said another way, the predictive distribution for a particular metric within a region does not change over the course of a season - it always yields the same predictive distribution based on historical data.

### Kernel Conditional Density Estimation (KCDE)
We use a method called kernel conditional density estimation (KCDE) to estimate predictive distributions for each target. 
KCDE is used to obtain separate predictive densities for each future week in the season. 
In order to predict seasonal quantities (onset, peak timing, and peak incidence), we use a copula to model dependence among those individual predicitive densities for each week, thereby obtaining a joint predicitive density for incidence in all future weeks.  
Predicitive densities for the seasonal quantities can be obtained as appropriate integrals of this joint density (see [[CITE RAY]] for details)

### Seasonal auto-regressive integrated moving average (SARIMA)
This model is fit to seasonally differenced wILI on the natural log scale, using an established stepwise procedure to select the model specification. [[From auto.arima? Should be specific and cite relevant R package.]] 

## Model training and cross-validation

We used data from 14 seasons (1997/1998 through 2010/2011) to train the models. Data from five seasons (2011/2012 through 2015/2016) were held out when fitting the models and used exclusively in the testing phase. Care was taken to ensure that our test predictions were made only once, to avoid overfitting our models \citep{Hastie2011}.

For each model-region pair, we fitted the model 14 times, each time excluding one season during the fitting.
Then for each season-week with the left-out season, we generated a set of three predictive distributions, one for each of the prediction targets. 
The predictive distributions were represented by probabilities assigned to bins associated with different possible outcomes.
For example, for onset week, the bins are each possible season week plus a bin for "no onset". 
Using the eventually observed value for each of the three outcomes, we then calculated the log-score achieved by each of the models at each season-week.
The log score is a proper scoring rule\citep{Gneiting2007}, calculated in our setting as the natural log of the probability assigned to the bin containing the true observation.

## Ensemble models 

Our ensemble framework can be described using the following notation.
Let $f_m(y|\bX)$ denote the predictive density from model $m$ for the value of the scalar random variable $Y$ conditional on observed variables $\bX$.  
For example, $Y$ could represent the peak incidence for a given season or the incidence in the season corresponding to a particular week.
In the context of repeated predictions from a time-series, the covariate vector $\bX$ may include time-dependent covariates, e.g. the week at which the prediction is made. 
However to introduce the method more generally, we can think of $\bX$ as being any set of covariates that may influence which model may be more tied to the true outcome.

The combined predictive density for a particular target in a given region can be written as 
\begin{align}
f(y|\bX) &= \sum_{m = 1}^M \pi_m f_m(y|\bX) \text{, where} \label{eqn:EnsembleModel} \\
\pi_{m}(\bX) &= \frac{\exp\{\rho_m(\bX)\}}{\sum_{m' = 1}^M \exp\{\rho_{m'}(\bX)\}}  \label{eqn:PiSoftmaxRho} 
\end{align}
where $\rho_m(\bX)$ is an estimated function that can depend on observed data. 

In Equation \eqref{eqn:EnsembleModel} the $\pi_m$ are the model weights, which we regard as functions of $\bX$.  
These weights must be non-negative and sum to $1$ across $m$.  
We ensure that these constraints are met by parameterizing the $\pi_m$ in terms of the softmax transformation of real-valued functions $\rho_m$ in Equation \eqref{eqn:PiSoftmaxRho}.  
We avoid identifiability problems by fixing $\rho_M(\bX) = 0$ for all $\bX$.  
For other models $m$, $\rho_m(\bX) > 0$ indicates that model $m$ has more weight than the baseline model for predictions at the given value of $\bX$. 

Using component model predictions from the training phase, we used four distinct techniques to create "stacked" models, or a probabilistic ensemble prediction. Based on exploration of training phase data only and a priori knowledge of the disease system, we chose three features of the system to illustrate the proposed "feature-weighting" methodology: week of season, model confidence (defined as the minimum number of predictive distribution bins required to cover 90% probability), and wILI measurement at the time of prediction.

1. Equal model weights, i.e. $\pi_m = 1/M$. In this scenario, each model contributes the same weight for each target and for all values of $\bX$.

2. Constant model weights, i.e. $\pi_m(\bX) = c_m$, a constant, where $\sum c_m = 1$ but the constants are not necessarily the same for each model. These weights are estimated using the degenerate estimation-maximization algorithm.\citep{Lin2004} 

3. Feature-weighted, i.e. $\rho_m(\bX)$ are estimated using gradient boossting with features including week of the season and model confidence. [[ do we include a season-week only model?]]

4. Feature-weighted and smoothed, i.e. $\rho_m(\bX)$ as above but with regularization tuning parameters chosen for the gradient boosting that penalize large fluctuations in the estimated $\rho_m(\bX)$. In these models, we include the same features as in 3 above along with the most recent wILI value.

All in all, this leads to [[6 or 7]] ensemble models. 
Each of the ensemble models, along with the three component models, are used to generate predictions in for each season-week of each of the five testing seasons, assuming perfect reporting. These predictions are then used to evaluate the prospective predictive performance of each of the ensemble methods. In total, we evaluate XX models in 11 regions over 5 years and 3 targets of interest. 

<!-- Two feature-weighted (changing by season_week only, estimated separately by metric and region) -->
<!--   3a. season_week -->
<!--   3b. season_week*model_confidence -->
<!-- Three feature-weighted with regularization parameters chosen via cross-validation: -->
<!--   4a. season_week -->
<!--   4b. season_week*model_confidence -->
<!--   4c. season_week*model_confidence*lag1_ili -->

<!-- X = (week of season, model confidence, recent incidence) -->

<!-- 8 methods (1 equal-bin-weight + 3 component + 4 ensemble) / 3-7 metrics / 5 test seasons -->

<!-- FeWe works regardless of discrete/continuous covariate -->

## Adaptively Weighted Ensemble framework

Preliminary investigations have revealed that there are nonlinearities in the relative performance of different models as a function of the time of year at which predictions are made. In Equation \eqref{eqn:rhoSplineExpansion} we capture these nonlinearities by parameterizing the functions $\rho_i$ using a ...

<!-- To prevent the stacking model from overfitting the training data, we estimate the model parameters by optimizing a penalized loss function where the penalty encourages the splines to be smooth.  The underlying unpenalized loss function is a measure of the quality of the final predictive distribution $f_t(y_t)$, obtained by cross-validation.  Because model performance will be evaluated using log scores, we will use negative cross-validated log scores as the loss function during parameter estimation: -->
<!-- The final predictions are obtained as a linear combination of the predictions from these component models.  The model weights depend on the week of the season in which the predictions are made.  We represent these weights as the softmax transformation of latent functions rho_ilt(season week) where i = 1, ..., 3 indexes the component model, l = 1, ..., 11 indexes the location (national or region 1 through 10), and t = 1, ..., 7 indexes the prediction target.  We estimate those latent functions rho_ilt via gradient tree boosting, optimizing leave-one-season-out crossvalidated log scores (using the definition of log scores specified for this competition). -->

We have introduced regularization into our model weighting scheme, which uses gradient tree boosting.  For now, weight regularization is done through three parameters; optimal values of those parameters are selected through cross-validation and a single best weighting scheme is selected.  The parameters are the depth of the regression trees used in the gradient tree boosting process, the number of boosting iterations, and the learning rate.

## Model testing


# Results


# Conclusion

General ensemble framework (not just time-series or infectious disease) 

Future Directions

 - evaluation for weekly incidence
 - standardize method into object-oriented framework for easy model plug-n-play
 - more complex models
 - use model weights to infer predictive value of different data sources (climate, strain-specific info)
 

# References
 
 
