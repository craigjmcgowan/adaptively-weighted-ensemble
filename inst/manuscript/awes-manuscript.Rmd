---
title: "Prediction of Infectious Disease Epidemics via Weighted Density Ensembles"
author: "Evan L Ray, Krzysztof Sakrejda, Nicholas G Reich"
output: 
    pdf_document:
        keep_tex: false
        citation_package: natbib
        fig_caption: true
date: "`r format(Sys.time(), '%B %Y')`"
header-includes:
   - \input{GrandMacros.tex}
   - \usepackage{setspace}\onehalfspacing
   - \usepackage{lineno}\linenumbers
   - \usepackage{booktabs}
   - \usepackage{xcolor,colortbl}
   - \renewcommand{\familydefault}{cmss}
   - \definecolor{gray1}{rgb}{0.01,0.01,0.01}
   - \definecolor{gray2}{rgb}{0.2,0.2,0.2}
   - \definecolor{gray3}{rgb}{0.3,0.3,0.3}
   - \definecolor{gray4}{rgb}{0.4,0.4,0.4}
   - \definecolor{gray5}{rgb}{0.5,0.5,0.5}
   - \definecolor{gray6}{rgb}{0.6,0.6,0.6}
   - \definecolor{gray7}{rgb}{0.7,0.7,0.7}
   - \definecolor{gray8}{rgb}{0.8,0.8,0.8}
   - \definecolor{gray9}{rgb}{0.9,0.9,0.9}
bibliography: feature-weighted-ensembles.bib
---

```{r init, include = FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(grid)
library(awes)
library(nlme)
library(multcomp)
library(mosaic)
library(gridExtra)
library(cowplot)
```

# Abstract


# Introduction

The practice of combining predictions from different models has been used for decades by climatologists and geophysical scientists. 
These methods have subsequently been adapted and extended by statisticians and computer scientists in diverse areas of scientific inquiry. 
In recent years, these "ensemble" forecasting approaches are frequently among the top methods used in prediction challenges across a wide range of applications.

Ensembles are a natural choice for noisy, complex, and interdependent systems that evolve over time. 
In these settings, no one model is likely to be able to capture and predict the full set of complex relationships that drive future observations from a particular system of interest. 
Instead "specialist" or "component" models can be relied on to capture distinct features or signals from a system which, when combined, represent a nearly complete range of possible outcomes. 

Using component models that generate predictive densities for outcomes of interest, we have implemented a series of ensembles using different methods for choosing the weights for each model.  
Specifically, we compare three different approaches. 
The first approach simply takes an equally weighted average of all models.
The second approach estimates constant but not necessarily equal weights for each model.
The third approach is a novel method for determining optimal weights based on features of the system at the time predictions are made.
The overarching goal of this study is to create a systematic comparison between increasingly complex ensemble methods to begin to study the benefits of including increasing complexity in ensemble weighting schemes.

This paper presents a novel ensemble method that determines optimal model combinations based on observed data and aspects of the predictive distributions obtained from those component models. 
We refer to models built using this approach as "feature-weighted" ensembles.
This approach fuses aspects of different ensemble methods: it uses model stacking \citep{Wolpert1992}, combines predictive densities as in Bayesian Model Averaging \citep{Madigan1998}, and estimates model weights based on features of the system \citep{Sill2009} using gradient tree boosting \citep{friedman2001greedy}.

To illustrate these ensemble methods, we present time-series forecasts for infectious disease, specifically for influenza in the U.S.
For millenia, infectious diseases have been one of the central existential threats to humankind.
The international significance of emerging epidemic threats in recent decades, including HIV/AIDS since the 1980s, SARS in 2003, H1N1 in 2009, and Ebola and MERS in 2014-2015, has also increased public awareness of the importance of understanding and being able to predict infectious disease dynamics.
With the revolution in data-driven science also occurring in the past few decades, there is now an increased focus on and hope for using statistics to inform public health policy and decision-making in ways that could impact future outbreaks. 
Some of the largest public health agencies in the world, including the US Centers for Disease Control and Prevention (CDC) have openly endorsed using models to inform decision making, saying "with models, decision-makers can look to the future with confidence in their ability to respond to outbreaks and public health emergencies."\citep{cdc-decisions-2016}

Development of the methods presented in this manuscript was motivated by the observation that certain prediction models for infectious disease consistently performed better than other models at certain times of year.  
We observed in earlier work that early in the U.S. influenza season, simple models of historical incidence often outperformed more standard time-series prediction models such as a seasonal auto-regressive integrated moving average (SARIMA) model\citep{ray2016}. 
However, in the middle of the season, the time-series models showed improved accuracy. 
We set out to determine whether ensemble methods could use this information about past model performance to improve predictions.

[[TODO: Insert 1-paragraph ensemble methods literature review here.]]

While there are many different methods for combining models, all ensemble models discussed in this paper use an approach called stacking.  
In this approach, each of the component models is trained separately in a first stage, and cross-validated measures of performance of those component models are obtained.  
Then, in a second stage, a stacking model is trained using the cross-validated performance measures to learn how to optimally combine predictions from the component models.

Using seasonal influenza outbreaks in the US health regions as a case-study, we developed and applied our ensemble models to predict several attributes of the influenza season at each week during the season.
By illustrating the utility of these approaches to ensemble forecasting in a setting with complex population dynamics, this work highlights the importance of continued innovation in ensemble methodology.

# Methods

This paper presents a comparison of methods for determining weights for stacked density ensembles, applied to forecasting specific features of influenza seasons in the U.S. 
First, we present a description of the influenza data we use in our application and the prediction targets.
Next, we discuss the three component models utilized by our ensemble framework.
Finally, we turn to the ensemble framework itself, describing the different ensemble model specifications used.

## Data and prediction targets

We obtained publicly available data on seasonal influenza activity in the United States between 1997 and 2016 from the U.S. Centers for Disease Control and Prevention (CDC) (Figure \ref{fig:raw-data}).
For each of the 10 Health and Human Services regions in the country in addition to the nation as a whole, the CDC calculates and publishes each week a measure called the weighted influenza-like illness (wILI) index.
The wILI for a particular region is calculated as the average proportion of doctor visits with influenza-like illness for each state in the region, weighted by state population. 
During the CDC-defined influenza season (between Morbidity and Mortality Weekly Report [MMWR] week 40 of one year and 20 of the next year), the CDC publishes updated influenza data on a weekly basis. 
This includes "current" wILI data from two weeks prior to the reporting date, as well as updates to previously reported numbers as new data becomes available. 
For this analysis, we use only the final reported wILI measures to train and predict from our models.

The CDC defines the influenza season onset as the first of three successive weeks of the season for which wILI is greater than or equal to a threshold that is specific to the region and season. 
This threshold is the mean percent of patient visits where the patient had ILI during low incidence weeks for that region in the past three seasons, plus two standard deviations \citep{cdc2016}.
The CDC provides historical threshold values for each region going back to the 2007/2008 season \citep{cdc2016-baselines}. 
Additionally, we define two other metrics specific to a region-season.
The peak incidence is the maximum observed wILI measured in a season. 
The peak week is the week at which the maximum wILI for the season is observed.

Each predictive distribution was represented by probabilities assigned to bins associated with different possible outcomes.
For onset week, the bins are represented by integer values for each possible season week plus a bin for "no onset". 
For peak week, the bins are represented by integer values for each possible season week.
For peak incidence, the bins capture incidence rounded to a single decimal place, with a single bin to capture all incidence over $13.05$. 
Formally, the incidence bins are as follows: [0, 0.05), [0.05, 0.15), ..., [12.95, 13.05), [13.05, $\infty$).  
These bins were used in the 2016-2017 influenza prediction contest run by the CDC (cite something).

We measure the accuracy of predictive distributions using the log score.
The log score is a proper scoring rule \citep{Gneiting2007}, calculated in our setting as the natural log of the probability assigned to the bin containing the true observation.
Proper scoring rules are preferred for measuring the quality of predictive distributions because the expected score is optimized by the true probabilty distribution.
We note that for peak week, in some region-seasons the same peak incidence was achieved in multiple weeks (after rounding to one decimal place).
In those cases, we calculated the log score as the log of the sum of the probabilities assigned to those weeks; this is consistent with scoring procedures used in the 2016-2017 flu prediction contest run by the CDC (cite something).

## Component models
We used three component models to generate probabilistic predictions of the three prediction targets. The first model was a seasonal average model that utilized kernel density estimation (KDE) to estimate a predictive distribution for each target. The second model utilized kernel conditional density estimation (KCDE) and copulas to create a joint predictive distribution for incidence in all remaining weeks of the season, conditional on recent observations of incidence.[[Cite Evan's paper on arxiv!]]  By calculating appropriate integrals of this joint distribution, we constructed predictive distributions for each of the seasonal targets.  The third model used a standard seasonal auto-regressive integrated moving average (SARIMA) implementation. All models were fit independently on data within each region.

### Kernel Density Estimation (KDE)
The simplest of the component models uses kernel density estimation \citep{silverman1986density} to estimate a distribution for each target based on observed values of that target in previous seasons within the region of interest.
We used Gaussian kernels and the default KDE settings from the `density` function in the `stats` package for R \citep{Rcore2016} to estimate the bandwidth parameter.
For the peak incidence target, we fit to log-transformed observations of historical peak incidence.
For the onset week prediction target, we estimated the probability of no onset as the proportion of region-seasons in all regions in the training phase where no week in the season met the criteria for being a season onset.

To create an empirical predictive distribution of size $N$ from a KDE fit based on a data vector $\by_{1:K}$ (for example, this might be the vector of peak week values from the $K$ training seasons), we first drew $N$ samples with replacement from $\by_{1:K}$, yielding a new vector $\tilde \by_{1:N}$.
We then drew a single psuedo-random deviates from each of $N$ truncated Gaussian distributions centered at $\tilde \by_{1:N}$ with the bandwidth estimated by the KDE algorithm.
The Gaussians we sampled from were truncated at the lower and upper bounds of possible values for the given prediction target.
Finally, we discretized the sampled values to the target-specific bins.
These sampled points then make up the empirical predictive distribution from a KDE model.
We set the sample size to $N = 10^5$.  In theory, this model assigns non-zero probability to every possible outcome; however, in a few cases the empirical predictive distribution resulting from this Monte Carlo sampling approach assigned probability zero to some of the bins.

It is important to note that the predictions from this model do not change as new data are observed over the course of the season.

### Kernel Conditional Density Estimation (KCDE)
We used kernel conditional density estimation and copulas to estimate a joint predictive distribution for flu incidence in each future week of the season, and then calculated predictive distributions for each target from that joint distribution.[[Cite Evan's paper]]
In our implementation, we first used KCDE to obtain separate predictive densities for flu incidence in each future week of the season.  Each of these predictive densities gives a conditional distribution for incidence at one future time point given recent observations of incidence and the current week of the season.  KCDE can be viewed as a distribution-based analogue of nearest-neighbors regression.  We then used a copula to model dependence among those individual predicitive densities, thereby obtaining a joint predicitive density, or a distribution of incidence trajectories in all future weeks.

To predict seasonal quantities (onset, peak timing, and peak incidence), we simulate $N = 10^5$ trajectories of disease incidence from this joint predictive distribution.
For each simulated incidence trajectory, we compute the onset week, peak week, and peak incidence.
We then aggregate these values to create predictive distributions for each target.
This procedure for obtaining predictive distributions for the targets of interest can be formally justified as an appropriate Monte Carlo integral of the joint predictive distribution for disease incidence in future weeks (see \citep{Ray2016} for details).

### Seasonal auto-regressive integrated moving average (SARIMA)
We fit seasonal ARIMA models \citep{Box2015} to wILI observations transformed to be on the natural log scale.  We manually performed first-order seasonal differencing and used the stepwise procedure from the `auto.arima` function in the `forecast` package \citep{Hyndman2008} for R to select the specification of the auto-regressive and moving average terms.

Similar to KCDE, forecasts were obtained by sampling $N = 10^5$ trajectories of wILI values over the rest of the season (using the `simulate.Arima` function from the `forecast` package), and predictive distributions of the targets were computed from these sampled trajectories as described above.

### Component model training

We used data from 14 seasons (1997/1998 through 2010/2011) to train the models. 
Data from five seasons (2011/2012 through 2015/2016) were held out when fitting the models and used exclusively in the testing phase. 
To avoid overfitting our models, we made predictions for the test phase only once \citep{Hastie2011}.

Estimation of the ensemble models (discussed in the next subsection) requires cross-validated measures of performance of each of the component models in order to accurately gauge their relative performance.
For each region, we estimated the parameters of each component model 15 times: 14 fits were obtained excluding one training season at a time, and another fit used all of the training data.
For each fit obtained leaving one season out, we generated a set of three predictive distributions, one for each of the prediction targets in the held-out season.
We were not able to generate predictions from the SARIMA and KCDE models for some seasons in the training phase because those models used lagged observations from previous seasons that were missing in our data set.
The component model fits based on all of the training data were used to generate predictions for the test phase.


## Ensemble models

All of the ensemble models we consider in this article work by averaging predictions from the component models to obtain the ensemble prediction.
Additionally, these methods are stacked model ensembles because they use leave-one-season-out predictions from the independently estimated component models as inputs to estimate the model weights.\citep{Wolpert1992}
We begin our discussion of ensemble methods with a general overview, introducing a common set of notation and giving a broad outline of the ensemble models we will use in this article.
We then describe our proposed feature-weighted stacking ensemble model specification in more detail.

### Overview of ensemble models

A single set of notation can be used to describe all of the ensemble frameworks implemented here.
Let $f_m(y|\bx^{(m)})$ denote the predictive density from component model $m$ for the value of the scalar random variable $Y$ conditional on observed variables $\bx^{(m)}$.  For example, $Y$ could represent the peak incidence for a given season and region.
In the context of time series predictions, the covariate vector $\bx^{(m)}$ may include time-varying covariates such as the week at which the prediction is made or lagged incidence; we suppress that dependence on time in our notation for the sake of simplicity.  The superscript $^{(m)}$ reflects the fact that each component model may use a different set of covariates.

The combined predictive density $f(y|\bx)$ for a particular target can be written as 
\begin{equation}
f(y|\bx) = \sum_{m = 1}^M \pi_m(\bx) f_m(y|\bx^{(m)}). \label{eqn:EnsembleModel} 
\end{equation}
In Equation \eqref{eqn:EnsembleModel} the $\pi_m$ are the model weights, which are allowed to vary as a function of observed features in $\bx$.  We define $\bx$ to be a vector of all observed quantities that are used by any of the component models or in calculating the model weights.  In order to guarantee that $f(y|\bx)$ is a probability distribution we require that $\sum_{m = 1}^M \pi_m(\bx) = 1$ for all $\bx$.
Figure \ref{fig:stacking-concept} illustrates the concept of stacking the predictive densities for each component model.

In the following subsection, we propose a framework for estimating _feature-dependent weights_ for a stacked ensemble model. 
By _feature-dependent_ we mean that the weights associated with different component models are driven by observed features or covariates.
Although we illustrate the method in the context of time-series predictions, the method could be used in any setting where we wish to combine distribution estimates from multiple models.
Features could include observed data from the system being predicted (such as recent wILI measurements or the time of year at which predictions are being made), observed data from outside the system (for example, recent weather observations), or features of the predictions themselves (e.g. summaries of the predictive distributions from the component models, such as a measure of spread in the distribution, or the time until a predicted peak).
Based on exploration of training phase data and _a priori_ knowledge of the disease system, we chose three features of the system to illustrate the proposed "feature-weighting" methodology: 
week of season, 
component model uncertainty (defined as the minimum number of predictive distribution bins required to cover 90% probability), and 
wILI measurement at the time of prediction.
These features were chosen prior to and not changed after implementing test-phase predictions.

We used four distinct methodologies to define weights to use for the stacking models:

1. Equal Weights (\textbf{EW}): $\pi_m(\bx) = 1/M$. In this scenario, each model contributes the same weight for each target and for all values of $\bx$.

2. Constant model weights via degenerate EM (\textbf{dEM}): $\pi_m(\bx) = c_m$, a constant where $\sum_{m=1}^M c_m = 1$ but the constants are not necessarily the same for each model. These weights are estimated using the degenerate estimation-maximization (dEM) algorithm \citep{Lin2004}.  A separate set of weights is estimated for each region and prediction target.

3. Feature-weighted (\textbf{FW}): $\pi_m(\bx)$ depends on features including week of the season and model uncertainty for the KCDE and SARIMA models.  A separate set of weighting functions is estimated for each region and prediction target.

4. Feature-weighted with regularization: $\pi_m(\bx)$ depends on features, but with regularization discouraging the weights from taking extreme values or from varying too quickly as a function of $\bx$.  A separate set of weighting functions is estimated for each region and prediction target.  We fit three variations on this ensemble model, using different sets of features:
    a. (\textbf{FW-reg-w}) week of the season;
    b. (\textbf{FW-reg-wu}) week of the season and model uncertainty for the KCDE and SARIMA models;
    c. (\textbf{FW-reg-wui}) week of the season, model uncertainty for the KCDE and SARIMA models, and incidence (wILI) in the most recent week.

All in all, this leads to 6 ensemble models, summarized in Table \ref{tbl:EnsembleModelSummaryTable}.  The first three of these models (\textbf{EW}, \textbf{dEM}, and \textbf{FW}) can be viewed as variations on \textbf{FW-reg-wu} if we vary the amount and type of regularization imposed on the \textbf{FW-reg-wu} model.  Thus, comparisons among these four models will allow us to explore the benefits of allowing the model weights to depend on covariates while imposing an appropriate amount of rigidity on the model weight functions $\pi_m(\bx)$.  We will discuss the regularization strategies used in \textbf{FW-reg-wu} further in the next subsection.  Meanwhile, comparisons among the \textbf{FW-reg-w}, \textbf{FW-reg-wu}, and \textbf{FW-reg-wui} models will allow us to explore the relative contributions to predictive performance that can be achieved by allowing the model weights to depend on different features.

\begin{table}[hp]
\centering
\begin{tabular}{rcccccc}
\toprule
         & \multicolumn{6}{c}{Component Model Weights Vary with...} \\
\cline{2-7}
  &   & Prediction & Week of & SARIMA & KCDE & Current \\ 
Model & Region & Target & Season & Uncertainty & Uncertainty & wILI \\ 
  \hline
EW         &   &   &   &   &   &   \\
dEM        & X & X &   &   &   &   \\
FW         & X & X & X & X & X &   \\
FW-reg-w   & X & X & X &   &   &   \\
FW-reg-wu  & X & X & X & X & X &   \\
FW-reg-wui & X & X & X & X & X & X \\
\bottomrule
\end{tabular}
\caption{\label{tbl:EnsembleModelSummaryTable}Summary of ensemble methods and what the model weights depend on.}
\end{table}

Each of the six ensemble models, along with the three component models, are used to generate predictions in every season-week of each of the five testing seasons, assuming perfect reporting. 
These predictions are then used to evaluate the prospective predictive performance of each of the ensemble methods. 
In total, we evaluate 9 models in 11 regions over 5 years and 3 targets of interest. 

## Feature-weighted stacking framework

In this section we introduce the particular specification of the parameter weight functions $\pi_m(\bx)$ that we use and discuss estimation.

In order to ensure that the the $\pi_m$ are non-negative and sum to 1 for all values of $\bx$, we parameterize them in terms of the softmax transformation of real-valued latent functions $\rho_m$:  
\begin{equation}
\pi_{m}(\bx) = \frac{\exp\{\rho_m(\bx)\}}{\sum_{m' = 1}^M \exp\{\rho_{m'}(\bx)\}}.  \label{eqn:PiSoftmaxRho} 
\end{equation}
For a pair of models $l, m \in \{1, ..., M\}$, $\rho_l(\bx) > \rho_m(\bx)$ indicates that model $l$ has more weight than model $m$ for predictions at the given value of $\bx$.

The functions $\rho_m(\bx)$ could be parameterized and estimated using many different techniques, such as a linear specification in the features, splines, or so on.  We have chosen to estimate the functions $\rho_m(\bx)$ using gradient tree boosting.

Gradient tree boosting uses a forward stagewise additive modeling algorithm to iteratively and incrementally construct a series of regression trees that, when added together, create a function designed to minimize a given loss function. 
In our application, the algorithm builds up the $\rho_m(\bx)$ that minimize the negative cross-validated log-score of the stacked prediction $f(y|\bX)$.  We have used the `xgb.train` function in the `xgboost` package for `R` to perform this estimation.\citep{xgboost}

Specifically, we define a single tree as
\begin{equation}
T(\bx; \btheta) = \sum_{j=1}^J \gamma_j I_{R_j}(\bx),
\end{equation}
where the $R_j$ are a set of disjoint regions that comprise a partition of the space $\mathcal{X}$ of feature values $\bx$, and $I$ is the indicator function taking the value $1$ if $\bx \in R_j$ and $0$ otherwise. The parameters $\btheta = (\bpsi, \bgamma)$ for the tree are the split points $\bpsi$ partitioning $\mathcal{X}$ into the regions $R_j$ and the regression constants $\bgamma$ associated with each region.
The function $\rho_m(\bx)$ is obtained as the sum of $B$ trees:
\begin{equation}
\rho_m(\bx; \Theta_m) = \sum_{b=1}^B T(\bx; \btheta_{m, b}).
\end{equation}

In each iteration $b$ of the boosting process, we estimate $M$ new regression trees, one for each component model.  These trees are estimated so as to minimize a local approximation to the loss function around the weight functions that were obtained after the previous boosting iteration.

Gradient tree boosting is appealing as a method for estimating the functions $\rho_m$ because it offers a great deal of flexibility in how the weights can vary as a function of the features $\bx$.  On the other hand, this flexibility can lead to overfitting the training data.  In order to limit the chances of overfitting, we have explored the use of four regularization parameters:

1. The number of boosting iterations $B$.  As $B$ increases, more extreme weights (close to 0 or 1) and more rapid changes in the weights as $\bx$ varies are possible.

2. An $L_1$ penalty on the number of tree leaves, $J$.  A large penalty encourages the regression trees to have fewer leaves, so that there is less flexibility for the model weights to vary as a function of $\bx$.

3. The maximum depth of the regression trees; i.e., how many times the same region may be successively split.  Similarly to the $L_1$ penalty on the number of tree leaves, this controls how much flexibility there is for the model weights to vary in $\bx$.  Additionally, the maximum depth controls the number of interactions.  For instance, a maximum depth of 2 indicates that there can be at most two-way interactions between different features in setting model weights.

4. An $L_1$ penalty on the regression constants $\gamma_j$.  A large penalty encourages these constants to be small, so that the overall model weights change less in each boosting iteration.

We selected values for these regularization parameters using a grid search optimizing leave-one-season-out cross-validated model performance.

<!-- To prevent the stacking model from overfitting the training data, we estimate the model parameters by optimizing a penalized loss function where the penalty encourages the splines to be smooth.  The underlying unpenalized loss function is a measure of the quality of the final predictive distribution $f_t(y_t)$, obtained by cross-validation.  Because model performance will be evaluated using log scores, we will use negative cross-validated log scores as the loss function during parameter estimation: -->
<!-- The final predictions are obtained as a linear combination of the predictions from these component models.  The model weights depend on the week of the season in which the predictions are made.  We represent these weights as the softmax transformation of latent functions rho_ilt(season week) where i = 1, ..., 3 indexes the component model, l = 1, ..., 11 indexes the location (national or region 1 through 10), and t = 1, ..., 7 indexes the prediction target.  We estimate those latent functions rho_ilt via gradient tree boosting, optimizing leave-one-season-out crossvalidated log scores (using the definition of log scores specified for this competition). -->


<!-- Two feature-weighted (changing by season_week only, estimated separately by metric and region) -->
<!--   3a. season_week -->
<!--   3b. season_week*model_confidence -->
<!-- Three feature-weighted with regularization parameters chosen via cross-validation: -->
<!--   4a. season_week -->
<!--   4b. season_week*model_confidence -->
<!--   4c. season_week*model_confidence*lag1_ili -->

<!-- X = (week of season, model confidence, recent incidence) -->

<!-- 8 methods (1 equal-bin-weight + 3 component + 4 ensemble) / 3-7 metrics / 5 test seasons -->




## Software and code

We used `r R.version.string` for all analyses.\citep{Rcore2016}
All data and code used for this analysis is freely available in an R package online at https://github.com/reichlab/adaptively-weighted-ensemble and may be installed in R directly.
Predictions generated in real-time with early development versions of this model during the 2016/2017 influenza season may be viewed at https://reichlab.github.io/flusight/.
To maximize reproducibility of our work, we have set seeds prior to running code that relies on stochastic simulations using the `rstream` package.\citep{Leydold2015}  
Additionally, the manuscript itself was dynamically generated using RMarkdown.

# Results

To evaluate overall model performance, we computed the average log-score for each model across all regions and/or test seasons.
Additionally for the peak wILI targets, we computed the average log-score for each model in the test seasons separately before and after the peak week.
Similarly for the onset week target, we computed the average log-score for each model in the test seasons before and after the onset week.

## Describe variation in log-scores of component models in training years (colored ribbon plot)

## Compare estimated FW model-weights to log-scores to show method is reasonable (plot to come)

## Interpret results of best overall FW model for each metric (plot to come)

## Comparison of model performance by log-scores (whisker plots)

## Comparison of ranked log-score performance (heatmap)

  - worst performing model always a component model, never KCDE
  - ensemble models, esp. regularized, show less variability
  - CW best ensemble across each metric

```{r raw_data_plot, echo=FALSE, fig.height=8, fig.cap="\\label{fig:raw-data}Plot of influenza data.  The full data include observations aggregated to the national level and for 10 smaller regions.  Here we plot only the data at the national level and in two of the smaller regions; data for the other regions are qualitatively similar.  Missing data are indicated with vertical grey lines.  The vertical red dashed lines indicate the cutoff time between the training and testing phases; 5 seasons of data were held out for testing."}
flu_data$region[flu_data$region == "X"] <- "National"
flu_data$region <- factor(flu_data$region,
  levels = c("National", paste0("Region ", 1:10)))

regions_to_plot <- c("National", "Region 1", "Region 7")
#regions_to_plot <- c("National", paste0("Region ", 1:10))
  
train_cutoff_ind <- min(which(flu_data$season == "2011/2012"))
train_cutoff_time <- flu_data$time[train_cutoff_ind]
train_cutoffs <- data.frame(
  region = regions_to_plot,
  train_cutoff_time = as.numeric(as.Date(train_cutoff_time))
)

ggplot() +
  geom_line(aes(x = as.Date(time), y = weighted_ili),
    data = flu_data[flu_data$region %in% regions_to_plot & flu_data$season %in% paste0(1997:2015, "/", 1998:2016), ]) +
  geom_vline(aes(xintercept = as.numeric(as.Date(time))),
    colour = "grey",
    data = flu_data[is.na(flu_data$weighted_ili) & flu_data$region %in% regions_to_plot & flu_data$season %in% paste0(1997:2015, "/", 1998:2016), ]) +
  geom_vline(aes(xintercept = train_cutoff_time),
    colour = "red",
    linetype = 2,
    data = train_cutoffs) +
  facet_wrap(~ region, ncol = 1) +
  scale_x_date() +
  xlab("Time") +
  ylab("Weighted Proportion of Doctor's Office Visits\nwith Influenza-like Illness\n") +
  ggtitle("Influenza Data by Region") +
  theme_bw(base_size = 11)
```

<!--#### Figure: 9 panel grid (3 component models x 3 metrics) showing a solid line for each region that represents the average log score across all seasons \label{fig:log-scores}-->

```{r stacking-concept, echo=FALSE, fig.height=3, fig.width=9, fig.keep='last', warning=FALSE, message=FALSE, fig.cap="\\label{fig:stacking-concept}Conceptual diagram of how the stacking models operate on probabilistic predictive distributions. The distributions illustrated here have density bins of 1 wILI unit, which differs from those used in the manuscript for illustrative purposes only. Panel A shows the predictive distributions from three component models. Panel B shows scaled versions of the distributions from A, after being multiplied by model weights. In Panel C, the scaled distributions are literally stacked to create the final ensemble predictive distribution."}
awes_path <- find.package("awes")

kde_full <- readRDS(file.path(awes_path,
  "estimation/loso-predictions/kde-National-2005-2006-loso-predictions.rds"))
kcde_full <- readRDS(file.path(awes_path,
  "estimation/loso-predictions/kcde-National-2005-2006-loso-predictions.rds"))
sarima_full <- readRDS(file.path(awes_path,
  "estimation/loso-predictions/sarima-National-2005-2006-loso-predictions.rds"))

## get indices
tmp_week <- 20
idx_kde <- which(kde_full$analysis_time_season_week==tmp_week)
idx_kcde <- which(kcde_full$analysis_time_season_week==tmp_week)
idx_sarima <- which(sarima_full$analysis_time_season_week==tmp_week)

kde_dens <- kde_full %>% 
    filter(analysis_time_season_week == tmp_week) %>%
    dplyr::select(starts_with("peak_inc_bin")) %>%
    as.numeric(.)

kcde_dens <- kcde_full %>% 
    filter(analysis_time_season_week == tmp_week) %>%
    dplyr::select(starts_with("peak_inc_bin")) %>%
    as.numeric(.)

sarima_dens <- sarima_full %>% 
    filter(analysis_time_season_week == tmp_week) %>%
    dplyr::select(starts_with("peak_inc_bin")) %>%
    as.numeric(.)

weights <- data_frame(
    mdl = factor(c("KDE", "KCDE", "SARIMA"), levels=c("KDE", "KCDE", "SARIMA")),
    weight = c(.2, .5, .3),
    x = 9,
    y=.4)

pred_dens <- data_frame(
    inc_bin = seq(0,13, .1),
    KDE = exp(kde_dens), 
    KCDE = exp(kcde_dens), 
    SARIMA = exp(sarima_dens)) %>%
    gather(
        key = mdl,
        value = probability,
        -inc_bin) %>%
    ## aggregate bins
    mutate(inc_bin = ceiling(inc_bin)) %>%
    group_by(mdl, inc_bin) %>%
    summarize(probability = sum(probability)) %>%
    left_join(weights) %>% ungroup() %>%
    mutate(mdl = factor(mdl, levels=c("KDE", "KCDE", "SARIMA")))

## make the base plot
p_base <- ggplot(pred_dens) + 
#    geom_col(aes(x=inc_bin, fill=mdl)) + 
    geom_bar(aes(x=inc_bin, fill=mdl), stat="identity") +
    scale_fill_brewer(palette="Set2") +
    xlab("peak incidence value (binned wILI)") +
    coord_cartesian(xlim=c(0, 11), ylim=c(0, 0.5)) +
    theme_minimal() + 
    theme(legend.position = "none", panel.grid.minor=element_blank())

## left hand tri-part panel
p1 <- p_base + aes(y=probability) + theme(strip.text.y = element_blank())+
    facet_grid(mdl~.) + ggtitle("A: Original predictions") +
    geom_text(data=weights, aes(x=8, y=y, label=mdl), hjust=0, size=5)

## center tri-part panel
p2 <- p_base + aes(y=probability * weight) + 
    facet_grid(mdl~.) + ggtitle("B: Weighted predictions") +
    theme(strip.text.y = element_blank(), axis.title.y = element_blank()) +
    geom_text(data=weights, parse=TRUE,
        aes(x=x, y=y, label=paste(expression(pi[m]), "==", weight)))

## make complicated right hand panel

## top and bottom RHS
blankPlot <- ggplot() + geom_blank(aes(x=c(0,1),y=c(0,1))) +
    cowplot::theme_nothing()

title_plot <- blankPlot + 
    geom_text(aes(x=0, y=.1,label="C: Stacked prediction"), hjust=0, size=4.5)

axis_plot <- blankPlot + 
    geom_text(aes(x=.5, y=1,label="peak incidence value (binned wILI)"), size=4)

## middle left
p3 <- p_base + aes(y=probability * weight) + 
    theme(axis.title.y = element_blank(), axis.title.x=element_blank()) 

p4 <- grid.arrange(title_plot, p3, axis_plot)

grid.arrange(p1, p2, p4, ncol=3)

```


```{r log_scores_vs_analysis_time, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="\\label{fig:ComponentModelLogScoresVsWeek}Mean, minimum, and maximum log scores achieved by each component model in each week of the season, summarizing across all seasons in both the train and test phases when all three component models produced predictions."}
awes_path <- find.package("awes")
color_palette <- c("#E69F00", "#56B4E9", "#009E73")

log_scores <- bind_rows(
    assemble_predictions(
      preds_path = file.path(awes_path, "estimation/loso-predictions"),
      regions = c("National", paste0("Region", 1:10)),
      models = c("kde", "kcde", "sarima"),
      prediction_targets = c("onset", "peak_week", "peak_inc"),
      prediction_types = c("log_score")
    ),
    assemble_predictions(
      preds_path = file.path(awes_path, "evaluation/test-predictions"),
      regions = c("National", paste0("Region", 1:10)),
      models = c("kde", "kcde", "sarima"),
      prediction_targets = c("onset", "peak_week", "peak_inc"),
      prediction_types = c("log_score")
    )
  ) %>%
  gather_("prediction_target", "log_score",
    paste0(c("onset", "peak_week", "peak_inc"), "_log_score")) %>%
  mutate(prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10))


summarized_log_scores <- log_scores %>%
  filter(analysis_time_season %in% paste0(1999:2015, "/", 2000:2016)) %>%
  group_by(model, region, prediction_target, analysis_time_season_week) %>%
  summarize(
    mean_log_score = mean(log_score),
    min_log_score = min(log_score),
    max_log_score = max(log_score)) %>%
  mutate(Model = toupper(model))
summarized_log_scores$prediction_target[summarized_log_scores$prediction_target == "onset"] <- "Onset Timing"
summarized_log_scores$prediction_target[summarized_log_scores$prediction_target == "peak_inc"] <- "Peak Incidence"
summarized_log_scores$prediction_target[summarized_log_scores$prediction_target == "peak_week"] <- "Peak Timing"
summarized_log_scores$prediction_target <- factor(summarized_log_scores$prediction_target,
  levels = c("Onset Timing", "Peak Timing", "Peak Incidence"))

regions_to_plot <- c("National", "Region1", "Region7")

ggplot() +
  geom_ribbon(
    aes(x = analysis_time_season_week,
      ymin = min_log_score,
      ymax = max_log_score,
      colour = Model,
      fill = Model,
      linetype = Model),
    alpha = 0.2,
    data = summarized_log_scores[summarized_log_scores$region %in% regions_to_plot, ]) +
  geom_line(
    aes(x = analysis_time_season_week,
      y = mean_log_score,
      colour = Model,
      linetype = Model),
    size = 1.5,
    data = summarized_log_scores[summarized_log_scores$region %in% regions_to_plot, ]) +
  scale_fill_manual(values = color_palette) +
  scale_colour_manual(values = color_palette) +
  facet_grid(region ~ prediction_target) +
  xlab("Week of Season at Analysis Time") +
  ylab("Log Score") +
  ggtitle("Log Scores vs. Week of Season at Analysis Time") +
  theme_bw()
```


<!--#### Figure: Example of log-scores and estimated weights from one region by season-week (x) for each model (color). Panel 1 has log-scores (y); Panel 2 has estimated weights (y) using degenerate EM, feature-weighted, and feature-weighted + smoothed \label{fig:example-weights}

#### Figure: test phase summary: 3 row-facets, one for each target, each model (color?) year (x) is a point with log-score (y) \label{fig:test-phase-log-score} -->


```{r test_phase_log_scores_summary_v1, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="Test phase log scores summary v1.  Log scores across all seasons/season weeks represented in box plots.  Log scores of -Infinity are represented with a plus sign at -10.", fig.show='hide'}
awes_path <- find.package("awes")

preds <- assemble_predictions(
  preds_path = file.path(awes_path, "evaluation/test-predictions"),
  models = c("kde", "kcde", "sarima", "equal_weights", "em_stacking", "xgb_stacking_unregularized", "xgb_stacking_reg_w", "xgb_stacking_reg_wu", "xgb_stacking_reg_wui"),
  prediction_targets = c("onset", "peak_week", "peak_inc"),
  prediction_types = "log_score"
) %>%
  filter(analysis_time_season_week %in% 10:40) %>%
  gather_("prediction_target", "log_score",
    c("onset_log_score", "peak_week_log_score", "peak_inc_log_score")) %>%
  mutate(
    prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10)
  )

preds$model[preds$model == "kde"] <- "KDE"
preds$model[preds$model == "kcde"] <- "KCDE"
preds$model[preds$model == "sarima"] <- "SARIMA"
preds$model[preds$model == "equal_weights"] <- "EW"
preds$model[preds$model == "em_stacking"] <- "CW"
preds$model[preds$model == "xgb_stacking_unregularized"] <- "FW-wu"
preds$model[preds$model == "xgb_stacking_reg_w"] <- "FW-reg-w"
preds$model[preds$model == "xgb_stacking_reg_wu"] <- "FW-reg-wu"
preds$model[preds$model == "xgb_stacking_reg_wui"] <- "FW-reg-wui"
preds$model <- factor(preds$model,
  levels = c("KDE", "KCDE", "SARIMA",
    "EW", "CW", "FW-wu",
    "FW-reg-w", "FW-reg-wu", "FW-reg-wui"))

preds$prediction_target[preds$prediction_target == "onset"] <- "Onset Timing"
preds$prediction_target[preds$prediction_target == "peak_inc"] <- "Peak Incidence"
preds$prediction_target[preds$prediction_target == "peak_week"] <- "Peak Timing"
preds$prediction_target <- factor(preds$prediction_target,
  levels = c("Onset Timing", "Peak Timing", "Peak Incidence"))

ggplot() +
  geom_boxplot(aes(x = model, y = log_score),
    data = preds) +
  geom_point(aes(x = model, y = log_score),
    colour = "red",
    shape = "+",
    size = 5,
    position = position_jitter(width = 1, height = 0),
    data = preds %>%
      filter(is.infinite(log_score)) %>%
      mutate(log_score = -10)) +
  facet_wrap(~ prediction_target, ncol = 1) +
  xlab("Model") +
  ylab("Log Score") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```


```{r test_phase_log_scores_summary_v2, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="Test phase log scores summary v2.  Point estimates and confidence intervals for mean log score for each model in weeks before the target (onset or peak) occurred.  Estimates are obtained from a mixed effects model with a separate fixed effect mean for the interaction of model and prediction target; random effects for each combination of region, season, model, and prediction target; and lag 1 autocorrelation nested within each combination of region, season, model, and prediction target.  The wider confidence interval bounds are simultaneous confidence intervals with an approximate familywise 95% coverage rate for all intervals.  The inner confidence intervals are calculated separately, with approximate coverage rates of 95%.  Log scores of -Infinity were truncated at -10 before fitting this model.  This model could be fit separately to obtain estimates for mean log scores in weeks on or after the target occurred."}
awes_path <- find.package("awes")

region_season_obs_quantities <- flu_data %>%
  select_(.dots = c("region", "season")) %>%
  distinct() %>%
  filter(season %in% paste0(2011:2015, "/", 2012:2016)) %>%
  mutate(
    observed_onset_week = NA,
    observed_peak_week = NA
  )

for(rs_row in seq_len(nrow(region_season_obs_quantities))) {
  temp <- get_observed_seasonal_quantities(
    data = flu_data[flu_data$region == region_season_obs_quantities$region[rs_row], , drop = FALSE],
    season = region_season_obs_quantities$season[rs_row],
    first_CDC_season_week = 10,
    last_CDC_season_week = 42,
    onset_baseline =
      get_onset_baseline(region = region_season_obs_quantities$region[rs_row],
        season = region_season_obs_quantities$season[rs_row]),
    incidence_var = "weighted_ili",
    incidence_bins = data.frame(
      lower = c(0, seq(from = 0.05, to = 12.95, by = 0.1)),
      upper = c(seq(from = 0.05, to = 12.95, by = 0.1), Inf)),
    incidence_bin_names = as.character(seq(from = 0, to = 13, by = 0.1))
  )
  
  region_season_obs_quantities$observed_onset_week[rs_row] <-
    temp$observed_onset_week
  region_season_obs_quantities$observed_peak_week[rs_row] <-
    temp$observed_peak_week[1]
}

region_season_obs_quantities$observed_onset_week[
  region_season_obs_quantities$observed_onset_week == "none"] <- 42
region_season_obs_quantities <- region_season_obs_quantities %>%
  transmute(
    region = gsub(" ", "", region),
    analysis_time_season = season,
    observed_onset_week = as.numeric(observed_onset_week),
    observed_peak_week = observed_peak_week
  )

all_models <- c("kde", "kcde", "sarima", "equal_weights", "em_stacking", "xgb_stacking_unregularized", "xgb_stacking_reg_w", "xgb_stacking_reg_wu", "xgb_stacking_reg_wui")
all_targets <- c("onset", "peak_week", "peak_inc")
preds <- assemble_predictions(
  preds_path = file.path(awes_path, "evaluation/test-predictions"),
  models = all_models,
  prediction_targets = all_targets,
  prediction_types = "log_score"
) %>%
  filter(analysis_time_season_week %in% 10:40) %>%
  gather_("prediction_target", "log_score",
    c("onset_log_score", "peak_week_log_score", "peak_inc_log_score")) %>%
  mutate(
    prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10),
    model = factor(model, levels = all_models),
    score = exp(log_score)
  )
preds$log_score[is.infinite(preds$log_score)] <- -10
preds <- preds %>%
  left_join(region_season_obs_quantities, by = c("region", "analysis_time_season")) %>%
  mutate(
    before_onset = (analysis_time_season_week < observed_onset_week),
    before_peak = (analysis_time_season_week < observed_peak_week))
preds$before_target_date <- ifelse(
  preds$prediction_target == "onset",
  c("on or after\ntarget date", "before\ntarget date")[preds$before_onset + 1],
  c("on or after\ntarget date", "before\ntarget date")[preds$before_peak + 1]
)

unique_region_season_model_autocor <- preds %>%
  filter(before_target_date == "before\ntarget date") %>%
  select_("region", "analysis_time_season", "model", "prediction_target", "log_score") %>%
  group_by_("region", "analysis_time_season", "model", "prediction_target") %>%
  summarize(
    log_score_autocor = acf(log_score, plot = FALSE)[["acf"]][2, 1, 1]
  )
## set to 1 for kde
unique_region_season_model_autocor$log_score_autocor[
  is.na(unique_region_season_model_autocor$log_score_autocor)] <- 1

preds <- preds %>%
  mutate(
    region = factor(region),
    analysis_time_season = factor(analysis_time_season),
    model = factor(model),
    prediction_target = factor(prediction_target),
    unique_region_season_model_target = factor(
      paste(region, analysis_time_season, model, prediction_target, sep = "_")
    )
  )
results_fit_before_target <- lme(log_score ~ model * prediction_target,
  random = ~ 1 | unique_region_season_model_target,
  correlation = corAR1(
    value = mean(unique_region_season_model_autocor$log_score_autocor),
    form = ~ analysis_time_season_week | unique_region_season_model_target),
  method = "REML",
  data = preds[preds$before_target_date == "before\ntarget date", ])

num_models <- length(all_models)
num_targets <- length(all_targets)

unique_model_descriptors <- paste0("model", all_models)
unique_target_descriptors <- paste0("prediction_target", all_targets)

lc_df <- expand.grid(
  model = all_models,
  target = all_targets,
  stringsAsFactors = FALSE)

lc_df$model_descriptor <- paste0("model", lc_df$model)
lc_df$target_descriptor <- paste0("prediction_target", lc_df$target)
lc_df$name <- apply(as.matrix(lc_df[, 1:2]), 1, paste, collapse = "-")

num_leading_cols <- ncol(lc_df)
coef_cols <- seq(
  from = num_leading_cols + 1,
  length = num_models * num_targets
)

# corresponding indicator vector for each coefficient
coef_names <- names(fixef(results_fit_before_target))
unique_coef_name_component_descriptors <- unique(unlist(strsplit(coef_names, ":")))
intercept_model <- unique_model_descriptors[
  !(unique_model_descriptors %in% unique_coef_name_component_descriptors)]
intercept_target <- unique_target_descriptors[
  !(unique_target_descriptors %in% unique_coef_name_component_descriptors)]
for(coef_ind in seq(from = 1, to = length(coef_names))) {
	split_name <- unlist(strsplit(coef_names[[coef_ind]], ":"))
	if(!any(split_name %in% unique_model_descriptors[unique_model_descriptors != intercept_model])) {
		split_name <- c(split_name, unique_model_descriptors)
	}
	if(!any(split_name %in% unique_target_descriptors[unique_target_descriptors != intercept_target])) {
		split_name <- c(split_name, unique_target_descriptors)
	}

	lc_df[[paste0("coef", coef_ind)]] <- 0
	lc_df[[paste0("coef", coef_ind)]][
	  lc_df$model_descriptor %in% split_name &
		lc_df$target_descriptor %in% split_name] <- 1
}

# ## contrasts of
# ## (mean performance CRF across data set and location) - (mean performance for each other method across data set and location),
# ## within response (type/intensity)
# for(fit_method in unique_fit_methods[unique_fit_methods != "CRF"]) {
#   for(response in unique_responses) {
#     rowind <- rowind + 1
#   	confint_rows <- c(confint_rows, rowind)
# 	  
#     crf_rowind <- which(lc_df$name == paste0("CRF", "-", response))
#     alt_rowind <- which(lc_df$name == paste0(fit_method, "-", response))
#     
#   	lc_df[rowind, ] <- rep(NA, ncol(lc_df))
#     lc_df$name[rowind] <- paste0("CRF", "-", fit_method, "-", response)
#   	lc_df$response[rowind] <- response
#   	lc_df[rowind, coef_cols] <- lc_df[crf_rowind, coef_cols] - lc_df[alt_rowind, coef_cols]
#   }
# }

lc_df$name <- factor(lc_df$name, levels = lc_df$name)

K_mat <- as.matrix(lc_df[, coef_cols])

# get point estimates
lc_df$pt_est <- as.vector(K_mat %*% matrix(fixef(results_fit_before_target)))

# get familywise CIs
confint_rows <- seq_len(nrow(lc_df))
lc_df$fam_CI_lb <- NA
lc_df$fam_CI_ub <- NA
fam_CI_obj <- glht(results_fit_before_target, linfct = K_mat[confint_rows, ])
temp <- confint(fam_CI_obj)$confint
lc_df$fam_CI_lb[confint_rows] <- temp[, 2]
lc_df$fam_CI_ub[confint_rows] <- temp[, 3]

# get individual CIs
lc_df$ind_CI_lb <- NA
lc_df$ind_CI_ub <- NA
for(rowind in confint_rows) {
	ind_CI_obj <- glht(results_fit_before_target, linfct = K_mat[rowind, , drop = FALSE])
	temp <- confint(ind_CI_obj)$confint
	lc_df$ind_CI_lb[rowind] <- temp[, 2]
	lc_df$ind_CI_ub[rowind] <- temp[, 3]
}


summary_figure_df <-
  lc_df[confint_rows, c("model", "target", "pt_est", "fam_CI_lb", "fam_CI_ub", "ind_CI_lb", "ind_CI_ub")]

summary_figure_df$model[summary_figure_df$model == "kde"] <- "KDE"
summary_figure_df$model[summary_figure_df$model == "kcde"] <- "KCDE"
summary_figure_df$model[summary_figure_df$model == "sarima"] <- "SARIMA"
summary_figure_df$model[summary_figure_df$model == "equal_weights"] <- "EW"
summary_figure_df$model[summary_figure_df$model == "em_stacking"] <- "CW"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_unregularized"] <- "FW-wu"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_w"] <- "FW-reg-w"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_wu"] <- "FW-reg-wu"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_wui"] <- "FW-reg-wui"
summary_figure_df$model <- factor(summary_figure_df$model,
  levels = c("KDE", "KCDE", "SARIMA",
    "EW", "CW", "FW-wu",
    "FW-reg-w", "FW-reg-wu", "FW-reg-wui"))

summary_figure_df$target[summary_figure_df$target == "onset"] <- "Onset Timing"
summary_figure_df$target[summary_figure_df$target == "peak_inc"] <- "Peak Incidence"
summary_figure_df$target[summary_figure_df$target == "peak_week"] <- "Peak Timing"
summary_figure_df$target <- factor(summary_figure_df$target,
  levels = c("Onset Timing", "Peak Timing", "Peak Incidence"))

ggplot(summary_figure_df) +
  geom_point(aes(x = model, y = pt_est)) +
  geom_errorbar(aes(x = model, ymin = fam_CI_lb, ymax = fam_CI_ub)) +
  geom_errorbar(aes(x = model, ymin = ind_CI_lb, ymax = ind_CI_ub), width = 0.2) +
  facet_wrap(~ target, ncol = 1) +
  xlab("Model") +
  ylab("Mean Log Score") +
# 	scale_y_continuous(limits = c(0.6, 1), expand = c(0, 0)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```




```{r test_phase_log_scores_summary_v2b, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="Test phase log scores summary v2.  Point estimates and confidence intervals for mean log score for each model in weeks on or after the target (onset or peak) occurred.  Estimates are obtained from a mixed effects model with a separate fixed effect mean for the interaction of model and prediction target; random effects for each combination of region, season, model, and prediction target; and lag 1 autocorrelation nested within each combination of region, season, model, and prediction target.  The wider confidence interval bounds are simultaneous confidence intervals with an approximate familywise 95% coverage rate for all intervals.  The inner confidence intervals are calculated separately, with approximate coverage rates of 95%.  Log scores of -Infinity were truncated at -10 before fitting this model.", fig.show='hide'}
awes_path <- find.package("awes")

region_season_obs_quantities <- flu_data %>%
  select_(.dots = c("region", "season")) %>%
  distinct() %>%
  filter(season %in% paste0(2011:2015, "/", 2012:2016)) %>%
  mutate(
    observed_onset_week = NA,
    observed_peak_week = NA
  )

for(rs_row in seq_len(nrow(region_season_obs_quantities))) {
  temp <- get_observed_seasonal_quantities(
    data = flu_data[flu_data$region == region_season_obs_quantities$region[rs_row], , drop = FALSE],
    season = region_season_obs_quantities$season[rs_row],
    first_CDC_season_week = 10,
    last_CDC_season_week = 42,
    onset_baseline =
      get_onset_baseline(region = region_season_obs_quantities$region[rs_row],
        season = region_season_obs_quantities$season[rs_row]),
    incidence_var = "weighted_ili",
    incidence_bins = data.frame(
      lower = c(0, seq(from = 0.05, to = 12.95, by = 0.1)),
      upper = c(seq(from = 0.05, to = 12.95, by = 0.1), Inf)),
    incidence_bin_names = as.character(seq(from = 0, to = 13, by = 0.1))
  )
  
  region_season_obs_quantities$observed_onset_week[rs_row] <-
    temp$observed_onset_week
  region_season_obs_quantities$observed_peak_week[rs_row] <-
    temp$observed_peak_week[1]
}

region_season_obs_quantities$observed_onset_week[
  region_season_obs_quantities$observed_onset_week == "none"] <- 42
region_season_obs_quantities <- region_season_obs_quantities %>%
  transmute(
    region = gsub(" ", "", region),
    analysis_time_season = season,
    observed_onset_week = as.numeric(observed_onset_week),
    observed_peak_week = observed_peak_week
  )

all_models <- c("kde", "kcde", "sarima", "equal_weights", "em_stacking", "xgb_stacking_unregularized", "xgb_stacking_reg_w", "xgb_stacking_reg_wu", "xgb_stacking_reg_wui")
all_targets <- c("onset", "peak_week", "peak_inc")
preds <- assemble_predictions(
  preds_path = file.path(awes_path, "evaluation/test-predictions"),
  models = all_models,
  prediction_targets = all_targets,
  prediction_types = "log_score"
) %>%
  filter(analysis_time_season_week %in% 10:40) %>%
  gather_("prediction_target", "log_score",
    c("onset_log_score", "peak_week_log_score", "peak_inc_log_score")) %>%
  mutate(
    prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10),
    model = factor(model),
    score = exp(log_score)
  )
preds$log_score[is.infinite(preds$log_score)] <- -10
preds <- preds %>%
  left_join(region_season_obs_quantities, by = c("region", "analysis_time_season")) %>%
  mutate(
    before_onset = (analysis_time_season_week < observed_onset_week),
    before_peak = (analysis_time_season_week < observed_peak_week))
preds$before_target_date <- ifelse(
  preds$prediction_target == "onset",
  c("on or after\ntarget date", "before\ntarget date")[preds$before_onset + 1],
  c("on or after\ntarget date", "before\ntarget date")[preds$before_peak + 1]
)

unique_region_season_model_autocor <- preds %>%
  filter(before_target_date == "on or after\ntarget date") %>%
  select_("region", "analysis_time_season", "model", "prediction_target", "log_score") %>%
  group_by_("region", "analysis_time_season", "model", "prediction_target") %>%
  summarize(
    log_score_autocor = acf(log_score, plot = FALSE)[["acf"]][2, 1, 1]
  )
## set to 1 for kde
unique_region_season_model_autocor$log_score_autocor[
  is.na(unique_region_season_model_autocor$log_score_autocor)] <- 1

preds <- preds %>%
  mutate(
    region = factor(region),
    analysis_time_season = factor(analysis_time_season),
    model = factor(model),
    prediction_target = factor(prediction_target),
    unique_region_season_model_target = factor(
      paste(region, analysis_time_season, model, prediction_target, sep = "_")
    )
  )
results_fit_before_target <- lme(log_score ~ model * prediction_target,
  random = ~ 1 | unique_region_season_model_target,
  correlation = corAR1(
    value = mean(unique_region_season_model_autocor$log_score_autocor),
    form = ~ analysis_time_season_week | unique_region_season_model_target),
  method = "REML",
  data = preds[preds$before_target_date == "on or after\ntarget date", ])

num_models <- length(all_models)
num_targets <- length(all_targets)

unique_model_descriptors <- paste0("model", all_models)
unique_target_descriptors <- paste0("prediction_target", all_targets)

lc_df <- expand.grid(
  model = all_models,
  target = all_targets,
  stringsAsFactors = FALSE)

lc_df$model_descriptor <- paste0("model", lc_df$model)
lc_df$target_descriptor <- paste0("prediction_target", lc_df$target)
lc_df$name <- apply(as.matrix(lc_df[, 1:2]), 1, paste, collapse = "-")

num_leading_cols <- ncol(lc_df)
coef_cols <- seq(
  from = num_leading_cols + 1,
  length = num_models * num_targets
)

# corresponding indicator vector for each coefficient
coef_names <- names(fixef(results_fit_before_target))
unique_coef_name_component_descriptors <- unique(unlist(strsplit(coef_names, ":")))
intercept_model <- unique_model_descriptors[
  !(unique_model_descriptors %in% unique_coef_name_component_descriptors)]
intercept_target <- unique_target_descriptors[
  !(unique_target_descriptors %in% unique_coef_name_component_descriptors)]
for(coef_ind in seq(from = 1, to = length(coef_names))) {
	split_name <- unlist(strsplit(coef_names[[coef_ind]], ":"))
	if(!any(split_name %in% unique_model_descriptors[unique_model_descriptors != intercept_model])) {
		split_name <- c(split_name, unique_model_descriptors)
	}
	if(!any(split_name %in% unique_target_descriptors[unique_target_descriptors != intercept_target])) {
		split_name <- c(split_name, unique_target_descriptors)
	}

	lc_df[[paste0("coef", coef_ind)]] <- 0
	lc_df[[paste0("coef", coef_ind)]][
	  lc_df$model_descriptor %in% split_name &
		lc_df$target_descriptor %in% split_name] <- 1
}

# ## contrasts of
# ## (mean performance CRF across data set and location) - (mean performance for each other method across data set and location),
# ## within response (type/intensity)
# for(fit_method in unique_fit_methods[unique_fit_methods != "CRF"]) {
#   for(response in unique_responses) {
#     rowind <- rowind + 1
#   	confint_rows <- c(confint_rows, rowind)
# 	  
#     crf_rowind <- which(lc_df$name == paste0("CRF", "-", response))
#     alt_rowind <- which(lc_df$name == paste0(fit_method, "-", response))
#     
#   	lc_df[rowind, ] <- rep(NA, ncol(lc_df))
#     lc_df$name[rowind] <- paste0("CRF", "-", fit_method, "-", response)
#   	lc_df$response[rowind] <- response
#   	lc_df[rowind, coef_cols] <- lc_df[crf_rowind, coef_cols] - lc_df[alt_rowind, coef_cols]
#   }
# }

lc_df$name <- factor(lc_df$name, levels = lc_df$name)

K_mat <- as.matrix(lc_df[, coef_cols])

# get point estimates
lc_df$pt_est <- as.vector(K_mat %*% matrix(fixef(results_fit_before_target)))

# get familywise CIs
confint_rows <- seq_len(nrow(lc_df))
lc_df$fam_CI_lb <- NA
lc_df$fam_CI_ub <- NA
fam_CI_obj <- glht(results_fit_before_target, linfct = K_mat[confint_rows, ])
temp <- confint(fam_CI_obj)$confint
lc_df$fam_CI_lb[confint_rows] <- temp[, 2]
lc_df$fam_CI_ub[confint_rows] <- temp[, 3]

# get individual CIs
lc_df$ind_CI_lb <- NA
lc_df$ind_CI_ub <- NA
for(rowind in confint_rows) {
	ind_CI_obj <- glht(results_fit_before_target, linfct = K_mat[rowind, , drop = FALSE])
	temp <- confint(ind_CI_obj)$confint
	lc_df$ind_CI_lb[rowind] <- temp[, 2]
	lc_df$ind_CI_ub[rowind] <- temp[, 3]
}


summary_figure_df <-
  lc_df[confint_rows, c("model", "target", "pt_est", "fam_CI_lb", "fam_CI_ub", "ind_CI_lb", "ind_CI_ub")]

summary_figure_df$model[summary_figure_df$model == "kde"] <- "KDE"
summary_figure_df$model[summary_figure_df$model == "kcde"] <- "KCDE"
summary_figure_df$model[summary_figure_df$model == "sarima"] <- "SARIMA"
summary_figure_df$model[summary_figure_df$model == "equal_weights"] <- "EW"
summary_figure_df$model[summary_figure_df$model == "em_stacking"] <- "CW"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_unregularized"] <- "FW-wu"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_w"] <- "FW-reg-w"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_wu"] <- "FW-reg-wu"
summary_figure_df$model[summary_figure_df$model == "xgb_stacking_reg_wui"] <- "FW-reg-wui"
summary_figure_df$model <- factor(summary_figure_df$model,
  levels = c("KDE", "KCDE", "SARIMA",
    "EW", "CW", "FW-wu",
    "FW-reg-w", "FW-reg-wu", "FW-reg-wui"))

summary_figure_df$target[summary_figure_df$target == "onset"] <- "Onset Timing"
summary_figure_df$target[summary_figure_df$target == "peak_inc"] <- "Peak Incidence"
summary_figure_df$target[summary_figure_df$target == "peak_week"] <- "Peak Timing"
summary_figure_df$target <- factor(summary_figure_df$target,
  levels = c("Onset Timing", "Peak Timing", "Peak Incidence"))

ggplot(summary_figure_df) +
  geom_point(aes(x = model, y = pt_est)) +
  geom_errorbar(aes(x = model, ymin = fam_CI_lb, ymax = fam_CI_ub)) +
  geom_errorbar(aes(x = model, ymin = ind_CI_lb, ymax = ind_CI_ub), width = 0.2) +
  facet_wrap(~ target, ncol = 1) +
  xlab("Model") +
  ylab("Mean Log Score") +
# 	scale_y_continuous(limits = c(0.6, 1), expand = c(0, 0)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```



```{r test_phase_log_scores_by_season_heatmap, echo=FALSE, cache=TRUE, fig.height=8, fig.cap="Model performance ranked by mean log score within each of the five test seasons for predictions made before the target (season onset or peak) occurred."}
awes_path <- find.package("awes")

region_season_obs_quantities <- flu_data %>%
  select_(.dots = c("region", "season")) %>%
  distinct() %>%
  filter(season %in% paste0(2011:2015, "/", 2012:2016)) %>%
  mutate(
    observed_onset_week = NA,
    observed_peak_week = NA
  )

for(rs_row in seq_len(nrow(region_season_obs_quantities))) {
  temp <- get_observed_seasonal_quantities(
    data = flu_data[flu_data$region == region_season_obs_quantities$region[rs_row], , drop = FALSE],
    season = region_season_obs_quantities$season[rs_row],
    first_CDC_season_week = 10,
    last_CDC_season_week = 42,
    onset_baseline =
      get_onset_baseline(region = region_season_obs_quantities$region[rs_row],
        season = region_season_obs_quantities$season[rs_row]),
    incidence_var = "weighted_ili",
    incidence_bins = data.frame(
      lower = c(0, seq(from = 0.05, to = 12.95, by = 0.1)),
      upper = c(seq(from = 0.05, to = 12.95, by = 0.1), Inf)),
    incidence_bin_names = as.character(seq(from = 0, to = 13, by = 0.1))
  )
  
  region_season_obs_quantities$observed_onset_week[rs_row] <-
    temp$observed_onset_week
  region_season_obs_quantities$observed_peak_week[rs_row] <-
    temp$observed_peak_week[1]
}

region_season_obs_quantities$observed_onset_week[
  region_season_obs_quantities$observed_onset_week == "none"] <- 42
region_season_obs_quantities <- region_season_obs_quantities %>%
  transmute(
    region = gsub(" ", "", region),
    analysis_time_season = season,
    observed_onset_week = as.numeric(observed_onset_week),
    observed_peak_week = observed_peak_week
  )

all_models <- c("kde", "kcde", "sarima", "equal_weights", "em_stacking", "xgb_stacking_unregularized", "xgb_stacking_reg_w", "xgb_stacking_reg_wu", "xgb_stacking_reg_wui")
all_targets <- c("onset", "peak_week", "peak_inc")
preds <- assemble_predictions(
  preds_path = file.path(awes_path, "evaluation/test-predictions"),
  models = all_models,
  prediction_targets = all_targets,
  prediction_types = "log_score"
) %>%
  filter(analysis_time_season_week %in% 10:40) %>%
  gather_("prediction_target", "log_score",
    c("onset_log_score", "peak_week_log_score", "peak_inc_log_score")) %>%
  mutate(
    prediction_target = substr(prediction_target, 1, nchar(prediction_target) - 10),
    score = exp(log_score)
  )

preds$log_score[is.infinite(preds$log_score)] <- -10
preds <- preds %>%
  left_join(region_season_obs_quantities, by = c("region", "analysis_time_season")) %>%
  mutate(
    before_onset = (analysis_time_season_week < observed_onset_week),
    before_peak = (analysis_time_season_week < observed_peak_week))

preds$before_target_date <- ifelse(
  preds$prediction_target == "onset",
  c("on or after\ntarget date", "before\ntarget date")[preds$before_onset + 1],
  c("on or after\ntarget date", "before\ntarget date")[preds$before_peak + 1]
)

preds$model[preds$model == "kde"] <- "KDE"
preds$model[preds$model == "kcde"] <- "KCDE"
preds$model[preds$model == "sarima"] <- "SARIMA"
preds$model[preds$model == "equal_weights"] <- "EW"
preds$model[preds$model == "em_stacking"] <- "CW"
preds$model[preds$model == "xgb_stacking_unregularized"] <- "FW-wu"
preds$model[preds$model == "xgb_stacking_reg_w"] <- "FW-reg-w"
preds$model[preds$model == "xgb_stacking_reg_wu"] <- "FW-reg-wu"
preds$model[preds$model == "xgb_stacking_reg_wui"] <- "FW-reg-wui"
preds$model <- factor(preds$model,
  levels = c("KDE", "KCDE", "SARIMA",
    "EW", "CW", "FW-wu",
    "FW-reg-w", "FW-reg-wu", "FW-reg-wui"))

preds$prediction_target[preds$prediction_target == "onset"] <- "Onset Timing"
preds$prediction_target[preds$prediction_target == "peak_inc"] <- "Peak Incidence"
preds$prediction_target[preds$prediction_target == "peak_week"] <- "Peak Timing"
preds$prediction_target <- factor(preds$prediction_target,
  levels = c("Onset Timing", "Peak Timing", "Peak Incidence"))

preds <- preds %>%
  mutate(
    region = factor(region),
    analysis_time_season = factor(analysis_time_season)
  )

res <- mean(log_score ~ model + prediction_target + analysis_time_season,
  data = preds[preds$before_target_date == "before\ntarget date", ])
temp <- strsplit(names(res), ".", fixed = TRUE) %>%
  unlist() %>%
  matrix(ncol = 3, byrow = TRUE)
res <- cbind(temp, res) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  `colnames<-`(c("model", "prediction_target", "season", "mean_log_score")) %>%
  `rownames<-`(NULL) %>%
  group_by_(.dots = c("prediction_target", "season")) %>%
  mutate(mean_log_score = as.numeric(mean_log_score),
    rank = factor(rank(-1 * as.numeric(mean_log_score))))
res$model <- factor(res$model,
  levels = c("KDE", "KCDE", "SARIMA",
    "EW", "CW", "FW-wu",
    "FW-reg-w", "FW-reg-wu", "FW-reg-wui"))

overall <- res %>% group_by(prediction_target, model) %>%
    summarize(
        season="Average",
        mean_log_score = mean(mean_log_score),
        rank = factor(round(mean(as.numeric(rank))), levels=1:9)
        ) %>% ungroup()

res_with_overall <- bind_rows(res, overall)


ggplot(data = res_with_overall, aes(x = model, y = season)) +
  geom_tile(aes(fill = rank)) +
  geom_text(aes(label=format(round(mean_log_score,2), nsmall=2))) +
  facet_wrap(~ prediction_target, ncol = 1) +
  scale_fill_manual("Model Rank",
    breaks = as.character(1:9),
    labels = as.character(1:9),
    values = rev(c("#b2182b", "#d6604d", "#f4a582", "#fddbc7", "#f7f7f7", "#d1e5f0", "#92c5de", "#4393c3", "#2166ac"))) +
  xlab("Model") +
  ylab("Season") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

<!--#### Figure: test phase summary: 3 row-facets, one for each target, each model (color?) year (x) is a point with MAE (y) \label{fig:test-phase-MAE}-->


# Conclusion

### Achievements

 - developed ensemble framework that makes [[better]] predictions on average than component models
 - ensemble method uses novel method to estimate feature-dependent weights
 - predictions disseminated and updated weekly
 
### Strengths/novelty

 - feature-weighting works regardless of discrete/continuous covariate
 - operates on predictive distributions, not just point-estimates
 - General framework (not just time-series or infectious disease) 
 - framework could be used to answer epidemiologically relevant questions: when do certain models contain information that substantially improve predictions, e.g. adding weather, mechanistic models, strain specific models, etc...

[[TODO: Discuss how varying some of these regularization parameters can give the simpler EW and dEM models??  Or ignore?  Or defer to supplement?]]

### Limitations

 - illustration only includes simple models and simple features

# References
 
 
