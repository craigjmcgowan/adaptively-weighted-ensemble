---
title: "Prediction of Infectious Disease Epidemics via Feature-Weighted Density Ensembles"
author: "Evan L Ray, Krzysztof Sakrejda, Nicholas G Reich"
date: "January 2017"
output: pdf_document
---


## Abstract


## Introduction

The practice of combining predictions from different models has been used for decades by climatologists and geophysical scientists. These methods have subsequently been adapted and extended by statisticians and computer scientists in a wide array of applications. In recent years, these "ensemble" forecasting approaches are frequently used among the top methods in prediction challenges. 

Ensembles are a natural choice for noisy and complex interdependent systems that evolve over time. In this setting, no one model is likely to be able to predict the full dynamics of a complex system. Instead "specialist" models could be relied on to capture distinct features or signals from a system which, when combined, represent a complete range of possible outcomes. 

Using specialist methods that generate predictive densities for outcomes of interest, we have developed a feature-weighted density ensemble method that estimates model weights as functions of observed data. Our approach fuses aspects of different ensemble methods: it uses model "stacking" \cite{Wolpert1992}, combines predictive densities as in Bayesian Model Averaging \cite{Madigan1998}, and uses feature weighting from observed data \cite{Sill2009}. 

Mention gradient tree boosting too.

To illustrate this method, we present time-series forecasts for infectious disease, specifically for influenza in the U.S.
For millenia, infectious diseases have been one of the central existential threats to humankind. Even in today's modern and scientific world, the spectre of a global pandemic appears to be a real possibility. The international significance of emerging epidemic threats in recent decades, including HIV/AIDS since the 1980s, SARS in 2003, H1N1 in 2009, and Ebola and MERS in 2014-2015, has also increased public awareness of the importance of understanding and being able to predict infectious disease dynamics. With the revolution in data-driven science also occurring in the past few decades, there is now an increased focus on and hope for using statistics to inform public health policy and decision-making in ways that could impact future outbreaks. Some of the largest public health agencies in the world, including the US Centers for Disease Control and Prevention (CDC) have openly endorsed using models to inform decision making, saying: "with models, decision-makers can look to the future with confidence in their ability to respond to outbreaks and public health emergencies." [ADD CITATION: http://www.cdc.gov/cdcgrandrounds/archives/2016/january2016.htm]

The observation that motivated the development of the method presented in this manuscript was seeing how certain prediction models for infectious disease consistently performed better than other models at certain times of year. We set out to determine whether past model performance could improve predictions using feature-weighted model stacking. We observed that early in the U.S. influenza season, simple models of historical incidence often outperformed more standard time-series prediction models such as a seasonal auto-regressive integrated moving average (SARIMA) model. However, in the middle of the season, the time-series models showed improved accuracy.

Using seasonal influenza outbreaks in the US health regions as a case-study, we developed and applied our feature-weighted ensemble model to predicting several features of the influenza season at each week during the season. Specifically, we aimed to predict three values defined by the US Centers for Disease Control and Prevention (US CDC) that define important features of an epidemic season: 

 1. onset week: the week at which the incidence first exceeds and remains above a region and season specific threshold,
 2. peak week: the week at which the maximum incidence for the season is observed,
 3. peak incidence: the maximum observed weighted influenza-like illness (wILI) measure in a season.

## Methods

### Data

The season onset is defined as the first week of the season for which incidence in the following three weeks is greater than or equal to a threshold that is specific to the region and season.  This threshold is the mean percent of patient visits where the patient had ILI during low incidence weeks for that region in the past three seasons, plus two standard deviations (cite https://www.cdc.gov/flu/weekly/overview.htm).  The CDC provides historical threshold values for each region going back to the 2007/2008 season (cite https://github.com/cdcepi/FluSight-forecasts/blob/master/wILI_Baseline.csv); for the purposes of training our models, we have used region-specific average baseline values for the earlier seasons.

Is current implementation holding CDC baseline constant across seasons? predict_region_kde makes me think so.

### Component models
We used three component models to generate predictions of these quantities of interest. The first model was a seasonal average model that utilized kernel density estimation to estimate a predictive distribution for each target. The second model utilized kernel conditional density estimation to create predicted trajectories of incidence. By aggregating across all trajectories, we constructed predictive distributions for each target. The third model used a standard SARIMA implementation and model selection approach to fit a seasonal auto-regressive model to the data.

 1) A "fixed" model using either Kernel Density Estimation (for season onset week, season peak week, and season peak incidence), or a GAM (for predictions of incidence at horizons 1 to 4 weeks).  The predictions from this model do not change as new data are observed over the course of the season (though the predictions for incidence in individual weeks do depend on the week being predicted), and can be interpreted as a representation of "everything that we have seen in the past".  A separate model fit is obtained for each region. [[include trunc KDE?: Model number 1 in the original description (below) has been modified to truncate the predictive distributions for onset timing, peak timing, and peak incidence so that values that have been eliminated by previously observed incidence are assigned low probability.  This is done in a very ad hoc manner.  The predictive distribution for the remaining bins is scaled up so that the sum of the predictive probability assigned to all bins equals 1.]]

 2) A model combining Kernel Conditional Density Estimation (KCDE) and copulas.  KCDE is used to obtain separate predictive densities for each future week in the season.  In order to predict seasonal quantities (onset, peak timing, and peak incidence), we use a copula to model dependence among those individual predicitive densities, thereby obtaining a joint predicitive density for incidence in all future weeks.  Predicitive densities for the seasonal quantities can be obtained as appropriate integrals of this joint density.  A separate model fit is obtained for each region.

 3) A SARIMA model.  This model is fit to seasonally differenced log(weighted_ili) using a stepwise procedure to select the model specification.  A separate model fit is obtained for each region.

### Ensemble models 

1. Equal weights
2. Degenerate EM estimated weights (constant by X, estimated separately by metric and region)
3. Feature-weighted (changing by X, estimated separately by metric and region)
4. Feature-weighted with regularization parameters chosen via cross-validation

X = (week of season, recent incidence)

8 methods (1 equal-bin-weight + 3 component + 4 ensemble) / 3-7 metrics / 5 test seasons

FeWe works regardless of discrete/continuous covariate

### Adaptively Weighted Ensemble framework

Let $f_m(y)$ denote the predictive density from model $m$ for the value of the scalar random variable $Y$.  For example, $Y$ could represent the peak incidence for a given season or the incidence in the season corresponding to a particular week.  The combined predictive density at time $t$ is
\begin{align}
f(y|x) &= \sum_{m = 1}^M \pi_m f_m(y|x) \text{, where} \label{eqn:EnsembleModel} \\
\pi_{m} &= \frac{\exp\{\rho_m(x)\}}{\sum_{m' = 1}^M \exp\{\rho_{m'}(x)\}}  \label{eqn:PiMultilogitRho} 
\end{align}
where $\rho_m(x)$ is XXX.

In the context of repeated predictions from a time-series, the covariate vector $x$ may include time-dependent covariates, e.g. the week at which the prediction is made. However to introduce the method more generally, we can think of $x$ as being any set of covariates that may influence which model may be more tied to the true outcome.
    
In Equation~\eqref{eqn:EnsembleModel} the $\pi_i$ are the model weights, which we regard as functions of $d(t)$.  These weights must be non-negative and sum to $1$ across $i$.  We ensure that these constraints are met by parameterizing the $\pi_i$ in terms of the softmax transformation of real-valued functions $\rho_i$ in Equation~\eqref{eqn:PiMultilogitRho}.  We avoid identifiability problems by fixing $\rho_N(d) = 0$ for all $d$.  For other models $i$, $\rho_i(d) > 0$ indicates that model $i$ has more weight than the baseline model for predictions at the given value of $d$.

Preliminary investigations have revealed that there are nonlinearities in the relative performance of different models as a function of the time of year at which predictions are made. In Equation~\eqref{eqn:rhoSplineExpansion} we capture these nonlinearities by parameterizing the functions $\rho_i$ using a cubic B-spline with $K$ evenly spaced knots centered around a linear trend.  The $b_k(d)$ are the cubic B-spline basis functions.  Other flexible modeling tools such as Gaussian processes or gradient tree boosting could also be used.

To prevent the stacking model from overfitting the training data, we estimate the model parameters by optimizing a penalized loss function where the penalty encourages the splines to be smooth.  The underlying unpenalized loss function is a measure of the quality of the final predictive distribution $f_t(y_t)$, obtained by cross-validation.  Because model performance will be evaluated using log scores, we will use negative cross-validated log scores as the loss function during parameter estimation:
The final predictions are obtained as a linear combination of the predictions from these component models.  The model weights depend on the week of the season in which the predictions are made.  We represent these weights as the softmax transformation of latent functions rho_ilt(season week) where i = 1, ..., 3 indexes the component model, l = 1, ..., 11 indexes the location (national or region 1 through 10), and t = 1, ..., 7 indexes the prediction target.  We estimate those latent functions rho_ilt via gradient tree boosting, optimizing leave-one-season-out crossvalidated log scores (using the definition of log scores specified for this competition).

We have introduced regularization into our model weighting scheme, which uses gradient tree boosting.  For now, weight regularization is done through three parameters; optimal values of those parameters are selected through cross-validation and a single best weighting scheme is selected.  The parameters are the depth of the regression trees used in the gradient tree boosting process, the number of boosting iterations, and the learning rate.

### Model training and cross-validation

### Model testing


## Results


## Conclusion

General ensemble framework (not just time-series or infectious disease) 

Future Directions

 - evaluation for weekly incidence
 - standardize method into object-oriented framework for easy model plug-n-play
 - more complex models
 - use model weights to infer predictive value of different data sources (climate, strain-specific info)
 
 
 
 
